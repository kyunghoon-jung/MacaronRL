{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/sonic/Blanksheet/git_repo/JungKH/MacaronRL/Ray_tutorial/Vanila_Distributed_DQN',\n",
       " '/home/sonic/.conda/envs/RL_Env/lib/python37.zip',\n",
       " '/home/sonic/.conda/envs/RL_Env/lib/python3.7',\n",
       " '/home/sonic/.conda/envs/RL_Env/lib/python3.7/lib-dynload',\n",
       " '',\n",
       " '/home/sonic/.local/lib/python3.7/site-packages',\n",
       " '/home/sonic/.conda/envs/RL_Env/lib/python3.7/site-packages',\n",
       " '/home/sonic/.conda/envs/RL_Env/lib/python3.7/site-packages/IPython/extensions',\n",
       " '/home/sonic/.ipython']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Dueling Agent '''\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "from torchsummary import summary\n",
    "\n",
    "from qnetwork import QNetwork \n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "import wandb\n",
    "from subprocess import call\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, \n",
    "                 env: 'Environment',\n",
    "                 input_frame: ('int: the number of channels of input image'),\n",
    "                 input_dim: ('int: the width and height of pre-processed input image'),\n",
    "                 num_frames: ('int: Total number of training frames'),\n",
    "                 skipped_frame: ('int: The number of skipped frames in the environment'),\n",
    "                 eps_decay: ('float: Epsilon Decay_rate'),\n",
    "                 gamma: ('float: Discount Factor'),\n",
    "                 target_update_freq: ('int: Target Update Frequency (by frames)'),\n",
    "                 update_type: ('str: Update type for target network. Hard or Soft')='hard',\n",
    "                 soft_update_tau: ('float: Soft update ratio')=None,\n",
    "                 batch_size: ('int: Update batch size')=32,\n",
    "                 buffer_size: ('int: Replay buffer size')=1000000,\n",
    "                 update_start_buffer_size: ('int: Update starting buffer size')=50000,\n",
    "                 learning_rate: ('float: Learning rate')=0.0004,\n",
    "                 eps_min: ('float: Epsilon Min')=0.1,\n",
    "                 eps_max: ('float: Epsilon Max')=1.0,\n",
    "                 device_num: ('int: GPU device number')=0,\n",
    "                 rand_seed: ('int: Random seed')=None,\n",
    "                 plot_option: ('str: Plotting option')=False,\n",
    "                 model_path: ('str: Model saving path')='./',\n",
    "                 trained_model_path: ('str: Trained model path')=''):\n",
    "\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.device = torch.device(f'cuda:{device_num}' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        self.env = env\n",
    "        self.input_frames = input_frame\n",
    "        self.input_dim = input_dim\n",
    "        self.num_frames = num_frames\n",
    "        self.skipped_frame = skipped_frame\n",
    "        self.epsilon = eps_max\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_min = eps_min\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.update_cnt = 0\n",
    "        self.update_type = update_type\n",
    "        self.tau = soft_update_tau\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.update_start = update_start_buffer_size\n",
    "        self.seed = rand_seed\n",
    "        self.plot_option = plot_option\n",
    "        \n",
    "        self.q_behave = QNetwork((self.input_frames, self.input_dim, self.input_dim), self.action_dim).to(self.device)\n",
    "        self.q_target = QNetwork((self.input_frames, self.input_dim, self.input_dim), self.action_dim).to(self.device)\n",
    "        if trained_model_path:\n",
    "            self.q_behave.load_state_dict(torch.load(trained_model_path))\n",
    "            print(\"Trained model is loaded successfully.\")\n",
    "        self.q_target.load_state_dict(self.q_behave.state_dict())\n",
    "        self.q_target.eval()\n",
    "        self.optimizer = optim.Adam(self.q_behave.parameters(), lr=learning_rate) \n",
    "\n",
    "        self.memory = ReplayBuffer(self.buffer_size, (self.input_frames, self.input_dim, self.input_dim), self.batch_size)\n",
    "\n",
    "    def processing_resize_and_gray(self, frame):\n",
    "        ''' Network에 들어가는 이미지로 전처리를 해준다. 이 전처리는 DQN 논문을 참고하였다 '''\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) # Pure\n",
    "        # frame = cv2.cvtColor(frame[:177, 32:128, :], cv2.COLOR_RGB2GRAY) # Boxing\n",
    "        # frame = cv2.cvtColor(frame[2:198, 7:-7, :], cv2.COLOR_RGB2GRAY) # Breakout\n",
    "        frame = cv2.resize(frame, dsize=(self.input_dim, self.input_dim)).reshape(self.input_dim, self.input_dim).astype(np.uint8)\n",
    "        return frame \n",
    "\n",
    "    def get_init_state(self):\n",
    "\n",
    "        init_state = np.zeros((self.input_frames, self.input_dim, self.input_dim))\n",
    "        init_frame = self.env.reset()\n",
    "        init_state[0] = self.processing_resize_and_gray(init_frame)\n",
    "        \n",
    "        for i in range(1, self.input_frames): \n",
    "            action = self.env.action_space.sample()\n",
    "            for j in range(self.skipped_frame):\n",
    "                state, _, _, _ = self.env.step(action) \n",
    "            state, _, _, _ = self.env.step(action) \n",
    "            init_state[i] = self.processing_resize_and_gray(state) \n",
    "        return init_state\n",
    "\n",
    "    def get_state(self, state, action, skipped_frame=0):\n",
    "        '''\n",
    "        num_frames: how many frames to be merged\n",
    "        input_size: hight and width of input resized image\n",
    "        skipped_frame: how many frames to be skipped\n",
    "        '''\n",
    "        next_state = np.zeros((self.input_frames, self.input_dim, self.input_dim))\n",
    "        for i in range(len(state)-1):\n",
    "            next_state[i] = state[i+1]\n",
    "\n",
    "        rewards = 0\n",
    "        dones = 0\n",
    "        for _ in range(skipped_frame):\n",
    "            state, reward, done, _ = self.env.step(action) \n",
    "            rewards += reward \n",
    "            dones += int(done) \n",
    "        state, reward, done, _ = self.env.step(action) \n",
    "        next_state[-1] = self.processing_resize_and_gray(state) \n",
    "        rewards += reward \n",
    "        dones += int(done) \n",
    "        return rewards, next_state, dones\n",
    "\n",
    "    def select_action(self, state: 'Must be pre-processed in the same way while updating current Q network. See def _compute_loss'):\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.zeros(self.action_dim), self.env.action_space.sample()\n",
    "        else:\n",
    "            # if normalization is applied to the image such as devision by 255, MUST be expressed 'state/255' below.\n",
    "            state = torch.FloatTensor(state).to(self.device).unsqueeze(0)/255\n",
    "            Qs = self.q_behave(state)\n",
    "            action = Qs.argmax()\n",
    "            return Qs.detach().cpu().numpy(), action.detach().item()\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.memory.store(state, action, reward, next_state, done)\n",
    "\n",
    "    def update_current_q_net(self):\n",
    "        batch = self.memory.batch_load()\n",
    "        loss = self._compute_loss(batch)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def target_soft_update(self):\n",
    "        for target_param, current_param in zip(self.q_target.parameters(), self.q_behave.parameters()):\n",
    "            target_param.data.copy_(self.tau*current_param.data + (1.0-self.tau)*target_param.data)\n",
    "\n",
    "    def target_hard_update(self):\n",
    "        ''' DQN 논문은 이렇게 업데이트 하였다 '''\n",
    "        self.update_cnt = (self.update_cnt+1) % self.target_update_freq\n",
    "\n",
    "        # self.target_update_freq 만큼 step을 진행할 때마다 update를 한다.\n",
    "        if self.update_cnt==0:\n",
    "            self.q_target.load_state_dict(self.q_behave.state_dict())\n",
    "\n",
    "    def train(self):\n",
    "        tic = time.time()\n",
    "        losses = []\n",
    "        scores = []\n",
    "        epsilons = []\n",
    "        avg_scores = [[-1000]]\n",
    "\n",
    "        score = 0\n",
    "\n",
    "        print(\"Storing initial buffer..\")\n",
    "        state = self.get_init_state()\n",
    "\n",
    "        # 먼저 buffer에 self.update_start 개수만큼 데이터를 채운다.\n",
    "        for frame_idx in range(1, self.update_start+1):\n",
    "            _, action = self.select_action(state)\n",
    "            reward, next_state, done = self.get_state(state, action, skipped_frame=self.skipped_frame)\n",
    "            self.store(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done: state = self.get_init_state()\n",
    "\n",
    "        print(\"Done. Start learning..\")\n",
    "        history_store = []\n",
    "\n",
    "        # 학습 시작. 매 스텝마다 batch를 가지고 와서 학습.\n",
    "        for frame_idx in range(1, self.num_frames+1):\n",
    "            Qs, action = self.select_action(state)\n",
    "            reward, next_state, done = self.get_state(state, action, skipped_frame=self.skipped_frame)\n",
    "            self.store(state, action, reward, next_state, done)\n",
    "            history_store.append([state, Qs, action, reward, next_state, done])\n",
    "            loss = self.update_current_q_net()\n",
    "\n",
    "            if self.update_type=='hard':   self.target_hard_update()\n",
    "            elif self.update_type=='soft': self.target_soft_update()\n",
    "            \n",
    "            score += reward\n",
    "            losses.append(loss)\n",
    "\n",
    "            if done:\n",
    "                scores.append(score)\n",
    "                if np.mean(scores[-10:]) > max(avg_scores):\n",
    "                    torch.save(self.q_behave.state_dict(), self.model_path+'{}_Score:{}.pt'.format(frame_idx, np.mean(scores[-10:])))\n",
    "                    training_time = round((time.time()-tic)/3600, 1)\n",
    "                    np.save(self.model_path+'{}_history_Score_{}_{}hrs.npy'.format(frame_idx, score, training_time), np.array(history_store))\n",
    "                    print(\"          | Model saved. Recent scores: {}, Training time: {}hrs\".format(scores[-10:], training_time), ' /'.join(os.getcwd().split('/')[-3:]))\n",
    "                avg_scores.append(np.mean(scores[-10:]))\n",
    "\n",
    "                if self.plot_option=='inline': \n",
    "                    scores.append(score)\n",
    "                    epsilons.append(self.epsilon)\n",
    "                    self._plot(frame_idx, scores, losses, epsilons)\n",
    "                elif self.plot_option=='wandb': \n",
    "                    wandb.log({'Score': score, 'loss(10 frames avg)': np.mean(losses[-10:]), 'Epsilon': self.epsilon})\n",
    "                    print(score, end='\\r')\n",
    "                else: \n",
    "                    print(score, end='\\r')\n",
    "\n",
    "                score=0\n",
    "                state = self.get_init_state()\n",
    "                history_store = []\n",
    "            else: state = next_state\n",
    "\n",
    "            self._epsilon_step()\n",
    "\n",
    "        print(\"Total training time: {}(hrs)\".format((time.time()-tic)/3600))\n",
    "\n",
    "    def _epsilon_step(self):\n",
    "        ''' Epsilon decay control '''\n",
    "        eps_decay_list = [self.eps_decay, self.eps_decay/2.5, self.eps_decay/3.5, self.eps_decay/5.5] \n",
    "\n",
    "        if self.epsilon>0.30:\n",
    "            self.epsilon = max(self.epsilon-eps_decay_list[0], 0.1)\n",
    "        elif self.epsilon>0.25:\n",
    "            self.epsilon = max(self.epsilon-eps_decay_list[1], 0.1)\n",
    "        elif self.epsilon>1.7:\n",
    "            self.epsilon = max(self.epsilon-eps_decay_list[2], 0.1)\n",
    "        else:\n",
    "            self.epsilon = max(self.epsilon-eps_decay_list[3], 0.1)\n",
    "\n",
    "    # DQN agent의 loss를 구하는 식: Bellman optimality equation\n",
    "    def _compute_loss(self, batch: \"Dictionary (S, A, R', S', Dones)\"):\n",
    "        # If normalization is used, it must be applied to 'state' and 'next_state' here. ex) state/255\n",
    "        states = torch.FloatTensor(batch['states']).to(self.device) / 255\n",
    "        next_states = torch.FloatTensor(batch['next_states']).to(self.device) / 255\n",
    "        actions = torch.LongTensor(batch['actions'].reshape(-1, 1)).to(self.device)\n",
    "        rewards = torch.FloatTensor(batch['rewards'].reshape(-1, 1)).to(self.device)\n",
    "        dones = torch.FloatTensor(batch['dones'].reshape(-1, 1)).to(self.device)\n",
    "\n",
    "        current_q = self.q_behave(states).gather(1, actions)\n",
    "\n",
    "        next_q = self.q_target(next_states).max(dim=1, keepdim=True)[0].detach()\n",
    "        mask = 1 - dones\n",
    "        target = (rewards + (mask * self.gamma * next_q)).to(self.device)\n",
    "\n",
    "        loss = F.smooth_l1_loss(current_q, target)\n",
    "        return loss\n",
    "\n",
    "    def _plot(self, frame_idx, scores, losses, epsilons):\n",
    "        clear_output(True) \n",
    "        plt.figure(figsize=(20, 5), facecolor='w') \n",
    "        plt.subplot(131)  \n",
    "        plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
    "        plt.plot(scores) \n",
    "        plt.subplot(132) \n",
    "        plt.title('loss') \n",
    "        plt.plot(losses) \n",
    "        plt.subplot(133) \n",
    "        plt.title('epsilons')\n",
    "        plt.plot(epsilons) \n",
    "        plt.show() \n",
    "\n",
    "if __name__=='__main__':\n",
    "    agent = Agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
