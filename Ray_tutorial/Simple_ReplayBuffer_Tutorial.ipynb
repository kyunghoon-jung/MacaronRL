{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여기서는 Tutorial에서 배운 개념을 이용하여 간단하게 ReplayBuffer를 분산 환경에서 활용해보겠습니다. <br>즉, 아래와 같은 작업을 수행합니다. <br>  \n",
    "    1. 여럿의 agent(혹은 actor)가 공유 Replay Buffer에 경험데이터를 넣는다. \n",
    "    2. Learner는 batch만큼 그 공유 ReplayBuffer에서 load한 후 원하는 작업을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray \n",
    "import time \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 env를 정의하겠습니다. environment의 일반적인 메소드만 넣고 어떤 의미가 있는 행동이나 상태를 정의한 것은 아닙니다.\n",
    "class Env:        \n",
    "    def reset(self):\n",
    "        return np.ones((2,2))\n",
    "    \n",
    "    def step(self, action):\n",
    "        # state, reward, done 모두 random하게 지정. state의 크기는 2x2 차원을 가지는 2차원 메트릭스.\n",
    "        state = action*np.random.randn(2, 2)\n",
    "        reward = np.sum(state)\n",
    "        \n",
    "        # done은 numpy의 random.randn 이 0.06 보다 작을 때만 1을 주었습니다. 더 자주 done이 발생하도록 하고 싶다면, 0.06을 더 키우면 됩니다.\n",
    "        done = 1 if abs(np.random.randn())<0.06 else 0\n",
    "        return state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer를 정의합니다.\n",
    "class Buffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.state_buffer = np.zeros((buffer_size, 2 ,2))\n",
    "        self.action_buffer = np.zeros(buffer_size)\n",
    "        self.reward_buffer = np.zeros(buffer_size)\n",
    "        self.next_state_buffer = np.zeros((buffer_size, 2 ,2))\n",
    "        self.done_buffer = np.zeros(buffer_size)\n",
    "        self.act_idx_buffer = np.zeros(buffer_size)\n",
    "        \n",
    "        self.store_idx = 0\n",
    "        self.current_size = 0\n",
    "\n",
    "    def store(self, state, action, next_state, reward, done, actor_idx):\n",
    "        self.state_buffer[self.store_idx] = state\n",
    "        self.action_buffer[self.store_idx] = action\n",
    "        self.reward_buffer[self.store_idx] = reward\n",
    "        self.next_state_buffer[self.store_idx] = next_state\n",
    "        self.done_buffer[self.store_idx] = done\n",
    "        self.act_idx_buffer[self.store_idx] = actor_idx\n",
    "        \n",
    "        self.store_idx = (self.store_idx + 1) % self.buffer_size\n",
    "        self.current_size = min(self.current_size+1, self.buffer_size)\n",
    "    \n",
    "    def batch_load(self, batch_size): \n",
    "        indices = np.random.randint(self.store_idx, size=batch_size)  \n",
    "        return dict( \n",
    "                states=self.state_buffer[indices], \n",
    "                actions=self.action_buffer[indices],\n",
    "                rewards=self.reward_buffer[indices],\n",
    "                next_states=self.next_state_buffer[indices], \n",
    "                dones=self.done_buffer[indices],\n",
    "                actindices=self.act_idx_buffer[indices])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor의 역할은 각각 env에서 경험한 것을 buffer에 넘겨주는 역할을 합니다.\n",
    "@ray.remote\n",
    "class Actor:\n",
    "    def __init__(self, n_epi, learner, actor_idx):\n",
    "        self.env = Env() \n",
    "        self.n_epi = n_epi # 총 episode 수 입니다.\n",
    "        self.learner = learner # ray를 통해 공유하는 learner class입니다.\n",
    "        self.actor_idx = actor_idx # 어떤 actor에서 온 데이터인지 보기 위한 변수입니다.\n",
    "\n",
    "    def explore(self):\n",
    "        state = self.env.reset()\n",
    "        for i in range(1, self.n_epi+1):\n",
    "            time.sleep(0.1)\n",
    "            action = np.random.randint(10) \n",
    "            next_state, reward, done = self.env.step(action) \n",
    "            self.learner.store.remote(state, action, next_state, reward, done, self.actor_idx) \n",
    "            state = next_state\n",
    "            if done:\n",
    "                init_state = self.env.reset() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공유 Buffer를 통해 학습을 진행하는 Learner를 정의합니다.\n",
    "@ray.remote\n",
    "class Learner:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.memory = Buffer(buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    # __init__에서 정의된 replay buffer에 저장합니다. 이 메소드는 각 actor마다 실행됩니다. \n",
    "    def store(self, state, action, next_state, reward, done, actor_idx):\n",
    "        self.memory.store(state, action, next_state, reward, done, actor_idx)\n",
    "\n",
    "    # 저장된 buffer에서 데이터를 로딩합니다.\n",
    "    def update_network(self):\n",
    "        batch = self.memory.batch_load(self.batch_size)\n",
    "        loss = np.random.randn()\n",
    "        buf_current_size = self.memory.store_idx\n",
    "        return loss, batch['states'].shape, batch['actindices'], buf_current_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 5000\n",
    "learn_freq = 50\n",
    "batch_size = 16 \n",
    "\n",
    "learner = Learner.remote(buffer_size, batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epi = 1000\n",
    "num_actors = 16\n",
    "\n",
    "for idx in range(num_actors): \n",
    "    globals()[f'actors_{idx}'] = Actor.remote(n_epi, learner, idx)\n",
    "    globals()[f'actors_{idx}'].explore.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100): \n",
    "    time.sleep(1) \n",
    "    loss, batch_stat_shape, act_indices, buf_size = ray.get(learner.update_network.remote())\n",
    "    print(loss, batch_stat_shape, act_indices, buf_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
