{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분산 강화학습으로 CartPole을 DQN을 이용하여 구현해보겠습니다. <br>앞선 ReplayBuffer의 경우에 추가적으로 다음을 고려해야합니다. ReplayBuffer에서 설명했던 변수들은 설명을 생략하였습니다. <br>  \n",
    "    1. ReplayBuffer에서 Learner가 어떤 주기로 weight update를 할지\n",
    "    2. 각 Actor의 network parameter를 어떤 식으로 Learner로 부터 copy해 올지 --> 비동기? vs 동기? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray \n",
    "import gym\n",
    "import time \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-23 09:26:15,017\tINFO services.py:1173 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.61',\n",
       " 'raylet_ip_address': '192.168.0.61',\n",
       " 'redis_address': '192.168.0.61:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-01-23_09-26-14_572248_32208/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-01-23_09-26-14_572248_32208/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-01-23_09-26-14_572248_32208',\n",
       " 'metrics_export_port': 63176,\n",
       " 'node_id': '8c127e06df8f95b2f5bb56d151d1d197c2833618'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer를 정의합니다.\n",
    "@ray.remote\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, \n",
    "                 buffer_size: ('int: Buffer_size'), \n",
    "                 state_dim: ('tuple: State dim')):\n",
    "        \n",
    "        # 1차원 state라할지라도 tuple로 입력받도록 tuple 타입을 강제하였습니다. 밑에 줄의 self.buffer_dim을 구하기 위해서 이렇게 한 것인데요, 사실 빼도 상관없고 얼마든지 다르게 구현해도 무방합니다.\n",
    "        assert type(state_dim) == tuple \n",
    "        \n",
    "        self.buffer_dim = (buffer_size, ) + state_dim\n",
    "        self.buffer_size = buffer_size\n",
    "        self.state_buffer = np.zeros(self.buffer_dim)\n",
    "        self.action_buffer = np.zeros(buffer_size)\n",
    "        self.reward_buffer = np.zeros(buffer_size)\n",
    "        self.next_state_buffer = np.zeros(self.buffer_dim)\n",
    "        self.done_buffer = np.zeros(buffer_size)\n",
    "        self.act_idx_buffer = np.zeros(buffer_size)\n",
    "        \n",
    "        self.store_idx = 0\n",
    "        self.current_size = 0\n",
    "\n",
    "    def store(self, state, action, next_state, reward, done, actor_idx):\n",
    "        self.state_buffer[self.store_idx] = state\n",
    "        self.action_buffer[self.store_idx] = action\n",
    "        self.reward_buffer[self.store_idx] = reward\n",
    "        self.next_state_buffer[self.store_idx] = next_state\n",
    "        self.done_buffer[self.store_idx] = done\n",
    "        self.act_idx_buffer[self.store_idx] = actor_idx\n",
    "        \n",
    "        self.store_idx = (self.store_idx + 1) % self.buffer_size\n",
    "        self.current_size = min(self.current_size+1, self.buffer_size)\n",
    "    \n",
    "    def batch_load(self, batch_size): \n",
    "        indices = np.random.randint(self.store_idx, size=batch_size)  \n",
    "        return dict( \n",
    "                states=self.state_buffer[indices], \n",
    "                actions=self.action_buffer[indices], \n",
    "                rewards=self.reward_buffer[indices], \n",
    "                next_states=self.next_state_buffer[indices], \n",
    "                dones=self.done_buffer[indices],\n",
    "                actindices=self.act_idx_buffer[indices])  \n",
    "\n",
    "# buffer_size = 1000\n",
    "# batch_size = 16\n",
    "# state_dim = (4, )\n",
    "# temp_buffer = ReplayBuffer.remote(buffer_size, state_dim)\n",
    "# for i in range(50):\n",
    "#     temp_buffer.store.remote(np.array(state_dim), 1, np.array(state_dim), 1, 1, 1)\n",
    "# batch = temp_buffer.batch_load.remote(batch_size)\n",
    "# ray.get(batch)['actindices'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden=32):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        state_size = state_size[0]\n",
    "        self.fc1 = nn.Linear(state_size, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# state_size = (4, ) \n",
    "# action_size = 2 \n",
    "# temp_net = QNetwork(state_size, action_size, 32) \n",
    "# test = torch.randn(size=(4,)) \n",
    "# temp_net(test), temp_net(test).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor의 역할은 각각 env에서 경험한 것을 buffer에 넘겨주는 역할을 합니다.\n",
    "@ray.remote\n",
    "class Actor:\n",
    "    def __init__(self, \n",
    "                 learner: (\"class: Learner class\"),\n",
    "                 env_name: (\"str: Environment name\"), \n",
    "                 actor_idx: (\"int: The index of an actor\"), \n",
    "                 actor_update_freq: (\"int: Update frequency of an actor\"), \n",
    "                 epsilon: (\"int: starting epsilon value for e-greedy update\"), \n",
    "                 eps_decay: (\"int: epsilon decay rate\"), \n",
    "                 eps_min: (\"int: minimum epsilon value\"), \n",
    "                 hidden: (\"int: Update frequency of learner's q_behave network\"), \n",
    "                 device: (\"int: Cuda device number\")):\n",
    "        \n",
    "        self.env = gym.make(env_name)\n",
    "        self.learner = learner # ray를 통해 공유하는 learner class입니다.\n",
    "        self.actor_idx = actor_idx # 어떤 actor에서 온 데이터인지 보기 위한 변수입니다.\n",
    "        self.actor_update_freq = actor_update_freq # actor의 network weight를 얼마나 자주 업데이트 할 것인지 \n",
    "        self.device = device\n",
    "        \n",
    "        # DQN hyperparameters\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_min = eps_min\n",
    "\n",
    "        # Network parameters\n",
    "        self.state_dim = (self.env.observation_space.shape[0], )\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.q_behave = QNetwork(self.state_dim, self.action_dim, hidden).to(self.device)\n",
    "\n",
    "    def select_action(self, state): \n",
    "        # e-greedy로 action을 선택 \n",
    "        if np.random.random() < self.epsilon: \n",
    "            return np.zeros(self.action_dim), self.env.action_space.sample() \n",
    "        else: \n",
    "            state = torch.FloatTensor(state).to(self.device).unsqueeze(0) \n",
    "            Qs = self.q_behave(state) \n",
    "            action = Qs.argmax() \n",
    "            return Qs.detach().cpu().numpy(), action.detach().item() \n",
    "        \n",
    "    def explore(self):\n",
    "        score = 0\n",
    "        episodes_cnt = 0\n",
    "        state = self.env.reset()\n",
    "\n",
    "        # actor는 멈추지 않고 무한 loop로 exploration하도록 설정\n",
    "        while 1:\n",
    "            Qs, action = self.select_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(action) \n",
    "            self.learner.store.remote(state, action, next_state, reward, done, self.actor_idx) \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            self.epsilon = max(self.epsilon-self.eps_decay, self.eps_min)\n",
    "            if done:\n",
    "                state = self.env.reset() \n",
    "                score = 0\n",
    "                episodes_cnt = (episodes_cnt+1) % self.actor_update_freq\n",
    "                if episodes_cnt==0: self.get_weights()\n",
    "\n",
    "    def get_weights(self):\n",
    "        weight_copy = ray.get(self.learner.return_weights.remote())\n",
    "        print(weight_copy)\n",
    "        self.q_behave.load_state_dict(weight_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공유 Buffer를 통해 학습을 진행하는 Learner를 정의합니다. \n",
    "# Learner는 buffer에 있는 샘플을 이용하여 network parameter를 업데이트를 하며, agent에게 network weight을 전달합니다.\n",
    "\n",
    "@ray.remote\n",
    "class Learner:\n",
    "    def __init__(self, \n",
    "                 env_name: (\"str: Environment name\"),\n",
    "                 gamma: (\"float: Discount rate\"),\n",
    "                 buffer_size: (\"int: Buffer size\"), \n",
    "                 batch_size: (\"int: Batch size\"), \n",
    "                 update_buf_start: (\"int: Update starting buffer size\"), \n",
    "                 update_freq: (\"int: Update frequency of learner's q_behave network\"), \n",
    "                 update_target_freq: (\"int: Update frequency of learner's q_target network\"), \n",
    "                 hidden: (\"int: Update frequency of learner's q_behave network\"), \n",
    "                 learning_rate: (\"float: Learning rate for updating the q_behave network\"),\n",
    "                 device: (\"int: Cuda device number\")):\n",
    "        \n",
    "        # wandb init config \n",
    "        entity = 'rl_flip_school_team'  \n",
    "        project_name = 'Distributed_DQN'\n",
    "        run_idx = np.random.randint(1000)\n",
    "        wandb.init(\n",
    "                project=project_name, \n",
    "                entity=entity,\n",
    "                name=f'{run_idx}_Distributed_DQN'\n",
    "                ) \n",
    "\n",
    "        self.env = gym.make(env_name)\n",
    "        self.gamma = gamma\n",
    "        # Discrete action과 Box state인 경우\n",
    "        self.state_dim = (self.env.observation_space.shape[0], )\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        \n",
    "        self.memory = ReplayBuffer.remote(buffer_size, self.state_dim)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.update_cnt = 0 # q_behave 업데이트 횟수\n",
    "        self.update_freq = update_freq # q_behave 업데이트 주기\n",
    "        self.update_buf_start = update_buf_start # 업데이트 시작 buffer size\n",
    "        self.update_target_freq = update_target_freq # q_target 업데이트 주기\n",
    "        self.device = device\n",
    "        self.total_steps = 0\n",
    "        self.scores = []\n",
    "        self.losses = []\n",
    "\n",
    "        self.q_behave = QNetwork(self.state_dim, self.action_dim, hidden).to(self.device)\n",
    "        self.q_target = QNetwork(self.state_dim, self.action_dim, hidden).to(self.device)\n",
    "        self.q_target.load_state_dict(self.q_behave.state_dict())\n",
    "        self.q_target.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_behave.parameters(), lr=learning_rate) \n",
    "\n",
    "    # __init__에서 정의된 replay buffer에 저장합니다. 이 메소드는 각 actor마다 실행됩니다. \n",
    "    def store(self, state, action, next_state, reward, done, actor_idx):\n",
    "        self.total_steps += 1\n",
    "        self.memory.store.remote(state, action, next_state, reward, done, actor_idx)\n",
    "\n",
    "    # 저장된 buffer에서 데이터를 로딩한 후 q_network을 업데이트합니다.\n",
    "    def update_q_network(self):\n",
    "        # update_cnt를 q_behave를 업데이트 할 때마다 1씩 상승 (self.update_target_freq 만큼 q_behave를 업데이트를 할 때마다 q_target을 업데이트 하기 위함)\n",
    "        self.update_cnt = (self.update_cnt+1) % self.update_target_freq\n",
    "\n",
    "        batch = self.memory.batch_load.remote(self.batch_size)\n",
    "        batch = ray.get(batch)\n",
    "        loss = self._compute_loss(batch)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.losses.append(loss.item()) \n",
    "\n",
    "    def target_hard_update(self):  \n",
    "        # Hard update 방식\n",
    "        self.q_target.load_state_dict(self.q_behave.state_dict()) \n",
    "\n",
    "    def return_weights(self):\n",
    "        return self.q_behave.state_dict() # target network을 return할지 or behave network을 할지는 선택사항인 것 같습니다.\n",
    "\n",
    "    def select_action(self, state): \n",
    "        # e-greedy로 action을 선택 \n",
    "        test_epsilon = 0.05\n",
    "        if np.random.random() < test_epsilon: \n",
    "            return np.zeros(self.action_dim), self.env.action_space.sample() \n",
    "        else: \n",
    "            state = torch.FloatTensor(state).to(self.device).unsqueeze(0) \n",
    "            Qs = self.q_behave(state) \n",
    "            action = Qs.argmax() \n",
    "            return Qs.detach().cpu().numpy(), action.detach().item() \n",
    "\n",
    "    def train(self):\n",
    "        # 여기서는 training의 종료시점을 정하지 않았습니다.\n",
    "        print(\"training start..\")\n",
    "        state = self.env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        loop_cnt = 0\n",
    "        while 1:\n",
    "            loop_cnt += 1\n",
    "            # buffer에 어느 정도 sample이 쌓인 후에, 그리고 update_freq 마다 learner의 q_behave를 업데이트 합니다.\n",
    "            if (self.total_steps > self.update_buf_start) and ((self.total_steps%self.update_freq) == 0):\n",
    "                self.update_q_network()\n",
    "                Qs, action = self.select_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action) \n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    state = self.env.reset()\n",
    "                    print(score)\n",
    "                    self.scores.append(score)\n",
    "                    wandb.log({\"Score\": score})\n",
    "                    wandb.log({\"Loss\": round(np.mean(self.losses[-10:]),2)})\n",
    "                    score=0\n",
    "#                     self._plot_status()\n",
    "\n",
    "                # 만일 target_update_freq의 횟수 만큼 q_behave를 업데이트 했다면, target_network을 복사해옵니다.\n",
    "                if self.update_cnt==0: self.target_hard_update()\n",
    "\n",
    "    def _compute_loss(self, batch: \"Dictionary (S, A, R', S', Dones)\"):\n",
    "        states = torch.FloatTensor(batch['states']).to(self.device)\n",
    "        next_states = torch.FloatTensor(batch['next_states']).to(self.device)\n",
    "        actions = torch.LongTensor(batch['actions'].reshape(-1, 1)).to(self.device)\n",
    "        rewards = torch.FloatTensor(batch['rewards'].reshape(-1, 1)).to(self.device)\n",
    "        dones = torch.FloatTensor(batch['dones'].reshape(-1, 1)).to(self.device)\n",
    "\n",
    "        current_q = self.q_behave(states).gather(1, actions)\n",
    "        next_q = self.q_target(next_states).max(dim=1, keepdim=True)[0].detach()\n",
    "        mask = 1 - dones\n",
    "        target = (rewards + (mask * self.gamma * next_q)).to(self.device)\n",
    "\n",
    "        loss = F.smooth_l1_loss(current_q, target)\n",
    "        return loss\n",
    "\n",
    "    def _plot_status(self):\n",
    "        clear_output(True) \n",
    "        plt.figure(figsize=(20, 5), facecolor='w') \n",
    "        plt.subplot(121)  \n",
    "        plt.title(f'Score w.r.t. Total number of steps {self.total_steps}.')\n",
    "        plt.plot(self.scores) \n",
    "        plt.subplot(122) \n",
    "        plt.title('loss') \n",
    "        plt.plot(self.losses) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_lists = ['CartPole-v0']\n",
    "env_name = env_lists[0]\n",
    "gamma = 0.99\n",
    "\n",
    "buffer_size = 5000 # Replay Buffer 사이즈\n",
    "batch_size = 16    # Replay Buffer에서 가지고 올 샘플 개수\n",
    "update_buf_start = 100\n",
    "update_freq = 25\n",
    "update_target_freq = 10 \n",
    "\n",
    "hidden = 32\n",
    "learning_rate = 0.001\n",
    "# device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "\n",
    "learner = Learner.remote(env_name, gamma, buffer_size, batch_size, update_buf_start, update_freq, update_target_freq, hidden, learning_rate, device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectRef(a1d795c28184037adf5a1a820100000001000000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_actors 개수만큼 선언하고, explore 실행. actor라는 변수가 계속 중복이 되지만 실행은 잘 된다.\n",
    "num_actors = 20 # actor의 개수\n",
    "actor_update_freq = 2 # 몇 episode 만에 actor의 weight를 업데이트하는가\n",
    "epsilon = 1.0\n",
    "eps_decay = 0.0005\n",
    "eps_min = 0.1\n",
    "\n",
    "for actor_idx in range(num_actors):\n",
    "    actor = Actor.remote(learner, env_name, actor_idx, actor_update_freq, epsilon, eps_decay, eps_min, hidden, device)\n",
    "    actor.explore.remote()\n",
    "\n",
    "learner.train.remote()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
