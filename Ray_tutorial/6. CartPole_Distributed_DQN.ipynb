{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분산 강화학습으로 CartPole을 DQN을 이용하여 구현해보겠습니다. <br>기본적인 방식은 다음과 같습니다. <br>  \n",
    "    1. Replay Buffer: Actor로부터 data를 받고, Learner에게 data를 전달하는 역할\n",
    "    2. Parameter Server: Learner로부터 parameter를 받고, Actor에게 paramter를 전달하는 역할.\n",
    "    3. Learner: Replay Buffer로 부터 데이터를 받아 학습을 진행하고, Parameter Server로 Learner 모델의 parameter를 전달하는 역할.\n",
    "    4. Actor: Environment와 상호작용하며 data를 Replay Buffer에 전달하고, Parameter Server로부터 Learner 모델의 parameter를 받아 자신의 모델 parameter를 update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ISSUE<br>\n",
    "\n",
    "    cuda가 process에 올려진 객체를 ray.remote가 선언된 class의 변수로 전달할 경우, 에러가 발생합니다. \n",
    "    그래서 예를들어 Q-network은 @ray.remote로 데코레이션을 하지 않습니다. 마찬가지 이유로, Learner 또한 ray.remote를 하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray \n",
    "import gym\n",
    "import time \n",
    "import numpy as np \n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer를 정의합니다. \n",
    "@ray.remote \n",
    "class ReplayBuffer:\n",
    "    def __init__(self, \n",
    "                   buffer_size: ('int: Buffer_size'), \n",
    "                 state_dim: ('tuple: State dim')):\n",
    "\n",
    "        # 1차원 state라할지라도 tuple로 입력받도록 tuple 타입을 강제하였습니다. \n",
    "        # 밑에 줄의 self.buffer_dim을 구하기 위해서 이렇게 한 것인데요, 사실 빼도 상관없고 얼마든지 다르게 구현해도 무방합니다.\n",
    "        # 참고) ray를 쓸 때는, class선언시에 assert조건을 만족못하여도 에러를 주지 않습니다. class의 메소드를 실행하고나서야 __init__에서 assertion 에러가 있다고 표시를 합니다. \n",
    "        assert type(state_dim) == tuple\n",
    "        \n",
    "        self.buffer_dim = (buffer_size, ) + state_dim\n",
    "        self.buffer_size = buffer_size\n",
    "        self.state_buffer = np.zeros(self.buffer_dim)\n",
    "        self.action_buffer = np.zeros(buffer_size)\n",
    "        self.reward_buffer = np.zeros(buffer_size)\n",
    "        self.next_state_buffer = np.zeros(self.buffer_dim)\n",
    "        self.done_buffer = np.zeros(buffer_size)\n",
    "        self.act_idx_buffer = np.zeros(buffer_size)\n",
    "\n",
    "        self.store_idx = 0\n",
    "        self.current_size = 0\n",
    "        self.total_store_count = 0\n",
    "\n",
    "    def store(self, state, action, next_state, reward, done, actor_idx): \n",
    "        # actor_idx는 쓰이지 않지만, 중간에 데이터 저장이 잘 되는지 확인용 변수\n",
    "        self.state_buffer[self.store_idx] = state\n",
    "        self.action_buffer[self.store_idx] = action\n",
    "        self.reward_buffer[self.store_idx] = reward\n",
    "        self.next_state_buffer[self.store_idx] = next_state\n",
    "        self.done_buffer[self.store_idx] = done\n",
    "        self.act_idx_buffer[self.store_idx] = actor_idx\n",
    "        \n",
    "        self.total_store_count += 1 # used for counting the total number of steps during training\n",
    "        self.store_idx = (self.store_idx + 1) % self.buffer_size\n",
    "        self.current_size = min(self.current_size+1, self.buffer_size)\n",
    "    \n",
    "    def batch_load(self, batch_size): \n",
    "        indices = np.random.randint(self.current_size, size=batch_size)  \n",
    "        return dict( \n",
    "                states=self.state_buffer[indices], \n",
    "                actions=self.action_buffer[indices], \n",
    "                rewards=self.reward_buffer[indices], \n",
    "                next_states=self.next_state_buffer[indices], \n",
    "                dones=self.done_buffer[indices],\n",
    "                actindices=self.act_idx_buffer[indices])  \n",
    "    \n",
    "    # 아래의 메소드들은 ray로 다른 객체가 current_size, store_idx, total_store_count 변수들을 접근할 때 쓰기 위해서 선언\n",
    "    def return_current_size(self):\n",
    "        return self.current_size\n",
    "\n",
    "    def return_store_idx(self):\n",
    "        return self.store_idx\n",
    "\n",
    "    def return_total_store_count(self):\n",
    "        return self.total_store_count\n",
    "    \n",
    "# # test\n",
    "# buffer_size = 1000\n",
    "# batch_size = 16\n",
    "# state_dim = (4, )\n",
    "# temp_buffer = ReplayBuffer.remote(buffer_size, state_dim)\n",
    "\n",
    "# for i in range(50):\n",
    "#     temp_buffer.store.remote(np.array(state_dim), 1, np.array(state_dim), 1, 1, 1)\n",
    "\n",
    "# batch = temp_buffer.batch_load.remote(batch_size)\n",
    "# print(\"Batch Size:\", ray.get(batch)['actindices'].shape) \n",
    "\n",
    "# current_size = temp_buffer.return_current_size.remote()\n",
    "# print(\"Current Size: \", ray.get(current_size))\n",
    "\n",
    "# return_store_idx = temp_buffer.return_store_idx.remote()\n",
    "# print(\"Store Index: \", ray.get(return_store_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden=32):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        state_size = state_size[0]\n",
    "        self.fc1 = nn.Linear(state_size, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# state_size = (4, ) \n",
    "# action_size = 2 \n",
    "# temp_net = QNetwork(state_size, action_size, 32) \n",
    "# test = torch.randn(size=(4,)) \n",
    "# temp_net(test), temp_net(test).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Network_parameter_server:\n",
    "    def __init__(self): \n",
    "        self.is_saved = False # 언제부터 actor가 모델 parameter를 가지고 올 수 있는지 확인하는 변수\n",
    "        \n",
    "    def update_parameters(self, learner_params): \n",
    "        self.learner_params = learner_params\n",
    "        self.is_saved = True\n",
    "\n",
    "    def return_parameters(self):\n",
    "        self.is_saved = False\n",
    "        return self.learner_params\n",
    "        \n",
    "    def return_saving_status(self):\n",
    "        return self.is_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor의 역할은 각각 env에서 경험한 것을 buffer에 넘겨주는 역할을 합니다.\n",
    "@ray.remote\n",
    "class Actor:  \n",
    "    def __init__(self, \n",
    "                 params_server: (\"Class: Network parameter server\"),\n",
    "                 memory: (\"class: Replay Buffer\"),\n",
    "                 env_name: (\"str: Environment name\"), \n",
    "                 actor_idx: (\"int: The index of an actor\"), \n",
    "                 epsilon: (\"int: starting epsilon value for e-greedy update\"), \n",
    "                 eps_decay: (\"int: epsilon decay rate\"), \n",
    "                 eps_min: (\"int: minimum epsilon value\"), \n",
    "                 hidden: (\"int: Update frequency of learner's q_behave network\"), \n",
    "                 device: (\"int: Cuda device number\"),\n",
    "                 is_wandb: (\"str: Whether wandb is on or off\"),\n",
    "                 plot_mode: (\"str: whether to plot in wandb or inline in jupyter\"),\n",
    "                 WANDB_GROUP_NAME: (\"str: Wandb's group name for all actors\")):\n",
    "\n",
    "        # wandb init config \n",
    "        if is_wandb:\n",
    "            entity = 'rl_flip_school_team'  \n",
    "            project_name = 'Distributed_DQN'\n",
    "            wandb.init(\n",
    "                    group=WANDB_GROUP_NAME,\n",
    "                    project=project_name, \n",
    "                    entity=entity,\n",
    "                    name=f'{actor_idx}_Distributed_DQN'\n",
    "                    ) \n",
    "\n",
    "        self.env = gym.make(env_name)\n",
    "        self.params_server = params_server\n",
    "        self.memory = memory   # ray를 통해 공유하는 Replaybuffer class입니다.\n",
    "        self.actor_idx = actor_idx # 어떤 actor에서 온 데이터인지 보기 위한 변수입니다.\n",
    "        self.plot_mode = plot_mode\n",
    "        self.device = device\n",
    "\n",
    "        # DQN hyperparameters\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_min = eps_min\n",
    "\n",
    "        # Network parameters\n",
    "        self.state_dim = (self.env.observation_space.shape[0], )\n",
    "        try: self.action_dim = self.env.action_space.n # Discrete action\n",
    "        except: self.action_dim = env.action_space.shape[0] # Continous action            \n",
    "        self.q_behave = QNetwork(self.state_dim, self.action_dim, hidden).to(self.device)\n",
    "\n",
    "    def select_action(self, state): \n",
    "        # e-greedy로 action을 선택 \n",
    "        if np.random.random() < self.epsilon: \n",
    "            return np.zeros(self.action_dim), self.env.action_space.sample() \n",
    "        else: \n",
    "            state = torch.FloatTensor(state).to(self.device).unsqueeze(0) \n",
    "            Qs = self.q_behave(state) \n",
    "            action = Qs.argmax() \n",
    "            return Qs.detach().cpu().numpy(), action.detach().item() \n",
    "        \n",
    "    def explore(self):\n",
    "        score = 0\n",
    "        state = self.env.reset()\n",
    "\n",
    "        # actor는 멈추지 않고 무한 loop로 exploration하도록 설정\n",
    "        while 1:\n",
    "            Qs, action = self.select_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(action) \n",
    "            \n",
    "            self.memory.store.remote(state, action, next_state, reward, done, self.actor_idx) # 공유 ReplayBuffer에 저장\n",
    "            \n",
    "            score += reward\n",
    "            state = next_state\n",
    "            self.epsilon = max(self.epsilon-self.eps_decay, self.eps_min)\n",
    "            if done:\n",
    "                state = self.env.reset() \n",
    "                self._plot_status(score)\n",
    "                score = 0\n",
    "                buffer_status = ray.get(self.params_server.return_saving_status.remote())\n",
    "                if buffer_status: self._pull_parameters() \n",
    "\n",
    "    def _pull_parameters(self):\n",
    "        updated_params = ray.get(self.params_server.return_parameters.remote()) \n",
    "        self.q_behave.load_state_dict(updated_params) \n",
    "\n",
    "    def _plot_status(self, score):\n",
    "        if self.plot_mode=='wandb':\n",
    "            wandb.log({'Score': score, \n",
    "                       f'Score_{self.actor_idx}': score}, step=ray.get(self.memory.return_total_store_count.remote())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-29 12:22:12,544\tINFO services.py:1173 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.61',\n",
       " 'raylet_ip_address': '192.168.0.61',\n",
       " 'redis_address': '192.168.0.61:51760',\n",
       " 'object_store_address': '/tmp/ray/session_2021-01-29_12-22-11_640725_61040/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-01-29_12-22-11_640725_61040/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8266',\n",
       " 'session_dir': '/tmp/ray/session_2021-01-29_12-22-11_640725_61040',\n",
       " 'metrics_export_port': 64611,\n",
       " 'node_id': 'a533cab9e8c14ffa8bf131a77461560d14ee5a58'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "ray.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Test:\n",
    "    def __init__(self, plot_inline):\n",
    "        self.plot_inline = plot_inline\n",
    "        print(id(self.plot_inline)) \n",
    "    def gen_data(self):\n",
    "        for _ in range(4):\n",
    "            score = np.random.randn() \n",
    "            self.plot_inline.store_data(score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plot_inline:\n",
    "    def __init__(self):\n",
    "        globals()['scores'] = []\n",
    "\n",
    "    def plotting_inline(self):\n",
    "        plt.figure(facecolor='w')\n",
    "        plt.plot(scores) \n",
    "        plt.show() \n",
    "\n",
    "    def store_data(self, score):\n",
    "        scores.append(score)      \n",
    "        print(scores)\n",
    "        print(score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61201)\u001b[0m 140237806089888\n"
     ]
    }
   ],
   "source": [
    "plot_inline = Plot_inline()   \n",
    "test = Test.remote(plot_inline)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140025322169176"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(plot_inline) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectRef(cb230a572350ff44df5a1a820100000001000000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61201)\u001b[0m [0.7562328978135184]\n",
      "\u001b[2m\u001b[36m(pid=61201)\u001b[0m 0.7562328978135184\n",
      "\u001b[2m\u001b[36m(pid=61201)\u001b[0m [0.7562328978135184, 0.7444665207728387]\n",
      "\u001b[2m\u001b[36m(pid=61201)\u001b[0m 0.7444665207728387\n",
      "\u001b[2m\u001b[36m(pid=61201)\u001b[0m [0.7562328978135184, 0.7444665207728387, -0.9556071794407452]\n",
      "\u001b[2m\u001b[36m(pid=61201)\u001b[0m -0.9556071794407452\n",
      "\u001b[2m\u001b[36m(pid=61201)\u001b[0m [0.7562328978135184, 0.7444665207728387, -0.9556071794407452, 0.08973416355236462]\n",
      "\u001b[2m\u001b[36m(pid=61201)\u001b[0m 0.08973416355236462\n"
     ]
    }
   ],
   "source": [
    "test.gen_data.remote() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 100, 100, 100, 100]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "plot_inline.store_data(100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_inline.plotting_inline() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner는 buffer에 있는 샘플을 이용하여 network parameter를 업데이트를 하며, parameter server에 network weight을 전달합니다.\n",
    "# Learner는 network update 등 cuda 연산을 하고 cpu로 병렬처리하는 것이 없으므로, 즉 ray를 이용할 필요가 없으므로, @ray.remote를 활용하지 않습니다.\n",
    "class Learner: \n",
    "    def __init__(self, \n",
    "                 env_name: (\"str: Environment name\"),\n",
    "                 params_server: (\"Class: Network parameter server\"),\n",
    "                 memory: (\"class: ReplayBuffer\"),\n",
    "                 gamma: (\"float: Discount rate\"),\n",
    "                 update_buf_start: (\"int: Update starting buffer size\"), \n",
    "                 update_freq: (\"int: Frequency of updating learner's q_behave network\"), \n",
    "                 update_target_freq: (\"int: Frequency of updating learner's q_target network\"), \n",
    "                 update_push_freq: (\"int: Frequency of sending learner's paratemers to parameter-server\"), \n",
    "                 hidden: (\"int: Update frequency of learner's q_behave network\"), \n",
    "                 batch_size: (\"int: Batch size for updating network\"),\n",
    "                 learning_rate: (\"float: Learning rate for updating the q_behave network\"),\n",
    "                 device: (\"int: Cuda device number\"),\n",
    "                 is_wandb: (\"str: Whether wandb is on or off\"),\n",
    "                 plot_mode: (\"str: whether to plot in wandb or inline in jupyter\"),\n",
    "                 WANDB_GROUP_NAME: (\"str: Wandb's group name for all actors\")):\n",
    "                    \n",
    "        if is_wandb:\n",
    "            entity = 'rl_flip_school_team'  \n",
    "            project_name = 'Distributed_DQN'\n",
    "            wandb.init(\n",
    "                    group=WANDB_GROUP_NAME,\n",
    "                    project=project_name, \n",
    "                    entity=entity,\n",
    "                    name='Learner_Distributed_DQN'\n",
    "                    ) \n",
    "\n",
    "        self.env = gym.make(env_name)\n",
    "        self.params_server = params_server\n",
    "        self.memory = memory\n",
    "        self.gamma = gamma\n",
    "        self.plot_mode = plot_mode\n",
    "        \n",
    "        self.state_dim = (self.env.observation_space.shape[0], )\n",
    "        try: self.action_dim = self.env.action_space.n # Discrete action\n",
    "        except: self.action_dim = env.action_space.shape[0] # Continous action \n",
    "            \n",
    "        self.batch_size = batch_size\n",
    "        self.update_cnt = 0 # q_behave 업데이트 횟수\n",
    "        self.update_freq = update_freq # q_behave 업데이트 주기\n",
    "        self.update_buf_start = update_buf_start # 업데이트 시작 buffer size\n",
    "        self.update_target_freq = update_target_freq # q_target 업데이트 주기\n",
    "        self.update_push_freq = update_push_freq # parameter server에 보내는 주기\n",
    "        self.device = device\n",
    "        self.total_steps = 0\n",
    "        self.scores = []\n",
    "        self.losses = [0]\n",
    "\n",
    "        self.q_behave = QNetwork(self.state_dim, self.action_dim, hidden).to(self.device)\n",
    "        self.q_target = QNetwork(self.state_dim, self.action_dim, hidden).to(self.device)\n",
    "        self.q_target.load_state_dict(self.q_behave.state_dict())\n",
    "        self.q_target.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_behave.parameters(), lr=learning_rate) \n",
    "\n",
    "    # 저장된 buffer에서 데이터를 로딩한 후 q_network을 업데이트합니다.\n",
    "    def update_q_network(self):\n",
    "        # update_cnt를 q_behave를 업데이트 할 때마다 1씩 상승 (self.update_target_freq 만큼 q_behave를 업데이트를 할 때마다 q_target을 업데이트 하기 위함)\n",
    "        self.update_cnt += 1\n",
    "        batch = ray.get(self.memory.batch_load.remote(self.batch_size)) \n",
    "        loss = self._compute_loss(batch) \n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.losses.append(loss.item()) # for plotting the losses\n",
    "\n",
    "    def target_hard_update(self):  \n",
    "        # Hard update 방식\n",
    "        self.q_target.load_state_dict(self.q_behave.state_dict()) \n",
    "\n",
    "    def eval_select_action(self, state): \n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0) \n",
    "        Qs = self.q_behave(state) \n",
    "        action = Qs.argmax() \n",
    "        return Qs.detach().cpu().numpy(), action.detach().item() \n",
    "\n",
    "    def push_parameters(self):\n",
    "        # Send paramters to server \n",
    "        copied_model = deepcopy(self.q_behave).cpu()\n",
    "        self.params_server.update_parameters.remote(copied_model.state_dict())\n",
    "\n",
    "    def train(self):\n",
    "        # 여기서는 training의 종료시점을 정하지 않았습니다.\n",
    "        print(\"training start..\")\n",
    "        while 1:\n",
    "            self.update_q_network()\n",
    "#             learner_score = self.eval() \n",
    "            learner_score = 0\n",
    "            self._plot_status(learner_score)\n",
    "\n",
    "            # 만일 update_push_freq 횟수 만큼 q_behave를 업데이트 했다면, server에 parameter를 보냅니다.\n",
    "            if (self.update_cnt%self.update_push_freq)==0: self.push_parameters()\n",
    "\n",
    "            # 만일 target_update_freq의 횟수 만큼 q_behave를 업데이트 했다면, target_network을 복사해옵니다.\n",
    "            if (self.update_cnt%self.update_target_freq)==0: self.target_hard_update()\n",
    "                \n",
    "    def eval(self):\n",
    "        score = 0\n",
    "        state = self.env.reset()\n",
    "        while 1:\n",
    "            Qs, action = self.eval_select_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(action) \n",
    "            score += reward\n",
    "            state = next_state\n",
    "            if done: break\n",
    "        return score\n",
    "\n",
    "    def _compute_loss(self, batch: \"Dictionary (S, A, R', S', Dones)\"):\n",
    "        states = torch.FloatTensor(batch['states']).to(self.device)\n",
    "        next_states = torch.FloatTensor(batch['next_states']).to(self.device)\n",
    "        actions = torch.LongTensor(batch['actions'].reshape(-1, 1)).to(self.device)\n",
    "        rewards = torch.FloatTensor(batch['rewards'].reshape(-1, 1)).to(self.device)\n",
    "        dones = torch.FloatTensor(batch['dones'].reshape(-1, 1)).to(self.device)\n",
    "        \n",
    "        current_q = self.q_behave(states).gather(1, actions)\n",
    "        next_q = self.q_target(next_states).max(dim=1, keepdim=True)[0].detach()\n",
    "        mask = 1 - dones\n",
    "        target = (rewards + (mask * self.gamma * next_q)).to(self.device)\n",
    "        loss = F.smooth_l1_loss(target, current_q)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def _plot_status(self, score=0):\n",
    "        if self.plot_mode=='wandb':\n",
    "            wandb.log({\"Learner Score\": score, \n",
    "                       \"loss(10 frames avg)\": np.mean(self.losses[-10:])\n",
    "                      }, step=ray.get(self.memory.return_total_store_count.remote()))\n",
    "            \n",
    "#         if mode=='inline':\n",
    "#             clear_output(True)\n",
    "            \n",
    "#             subplot_params = [\n",
    "#                 (121, f\"Scores in episode_{i_episode}\", score_hist),\n",
    "#                 (122, f\"Policy loss in episode:{i_episode}\", loss_hist),\n",
    "#             ] self.total_store_cnt\n",
    "            \n",
    "#             plt.figure(figsize=(10, 5), facecolor='w')\n",
    "#             for loc, title, values in subplot_params:\n",
    "#                 plt.subplot(loc)\n",
    "#                 plt.title(f'Frame:{self.frame_cnt} '+title)\n",
    "#                 plt.plot(values)\n",
    "#             plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_lists = ['CartPole-v0', 'LunarLander-v2']\n",
    "env_name = env_lists[0]\n",
    "gamma = 0.99\n",
    "\n",
    "buffer_size = 20000 # Replay Buffer 사이즈\n",
    "batch_size = 64    # Replay Buffer에서 가지고 올 샘플 개수\n",
    "env = gym.make(env_name)\n",
    "state_dim = (env.observation_space.shape[0], ) \n",
    "        \n",
    "update_buf_start = 2000\n",
    "update_freq = 1\n",
    "update_target_freq = 10\n",
    "update_push_freq = 1\n",
    "\n",
    "hidden = 256\n",
    "learning_rate = 0.00005\n",
    "learner_device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "is_wandb = True     # whether to use wandb or not\n",
    "plot_mode = 'wandb' # plot options: 'wandb' or 'inline'\n",
    "WANDB_GROUP_NAME = 'Distributed_DQN_' + str(np.random.randint(10000))\n",
    "\n",
    "params_server = Network_parameter_server.remote() \n",
    "memory = ReplayBuffer.remote(buffer_size, state_dim)\n",
    "learner = Learner(env_name, params_server, memory, gamma, \n",
    "                  update_buf_start, update_freq, update_target_freq, update_push_freq, \n",
    "                  hidden, batch_size, learning_rate, learner_device, is_wandb, plot_mode, WANDB_GROUP_NAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# num_actors 개수만큼 선언하고, explore 실행. actor라는 변수가 계속 중복이 되더라도 실행은 잘 된다.\n",
    "num_actors = 15 # actor의 개수\n",
    "epsilon = 1.0\n",
    "eps_decay = 1/5000\n",
    "eps_min = 0.1\n",
    "actor_device = \"cpu\"\n",
    "\n",
    "for actor_idx in range(num_actors):\n",
    "    actor = Actor.remote(params_server, memory, env_name, actor_idx, \n",
    "                                                   epsilon, eps_decay, eps_min, hidden, actor_device, \n",
    "                                                   is_wandb, plot_mode, WANDB_GROUP_NAME)\n",
    "    actor.explore.remote() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while 1:\n",
    "    buffer_saved_cnt = ray.get(memory.return_total_store_count.remote())\n",
    "    if buffer_saved_cnt > learner.update_buf_start: learner.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
