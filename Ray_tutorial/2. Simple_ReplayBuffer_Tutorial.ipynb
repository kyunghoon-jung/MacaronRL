{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여기서는 Tutorial에서 배운 개념을 이용하여 간단하게 ReplayBuffer를 분산 환경에서 활용해보겠습니다. <br> 아래와 같은 작업을 수행합니다. <br>  \n",
    "    1. 여럿의 agent(혹은 actor)가 공유 Replay Buffer에 경험데이터를 넣는다. \n",
    "    2. Learner는 batch만큼 그 공유 ReplayBuffer에서 load한 후 원하는 작업을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 질문 <br>\n",
    "\n",
    "    1. Class의 method는 공유가 잘 되는데, class 안에 있는 __init__ 에서 선언된 variable은 불러올 수가 없었다. 어떻게 해야하는 걸까?\n",
    "       : 현재로써는 class 안에 변수를 전달하는 method를 따로 만들어서 ray.get으로 접근하는 방법을 쓰고있음.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray \n",
    "import time \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-22 01:44:20,295\tINFO services.py:1173 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.61',\n",
       " 'raylet_ip_address': '192.168.0.61',\n",
       " 'redis_address': '192.168.0.61:22655',\n",
       " 'object_store_address': '/tmp/ray/session_2021-01-22_01-44-19_756942_6987/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-01-22_01-44-19_756942_6987/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8266',\n",
       " 'session_dir': '/tmp/ray/session_2021-01-22_01-44-19_756942_6987',\n",
       " 'metrics_export_port': 51531,\n",
       " 'node_id': '7f142fba28c9e3ef830a7be287ebcfa20f43dcca'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 env를 정의하겠습니다. environment의 일반적인 메소드만 넣고 어떤 의미가 있는 행동이나 상태를 정의한 것은 아닙니다.\n",
    "class Env:        \n",
    "    def reset(self):\n",
    "        return np.ones((2,2))\n",
    "\n",
    "    def step(self, action):\n",
    "        # state, reward, done 모두 random하게 지정. state의 크기는 2x2 차원을 가지는 2차원 메트릭스.\n",
    "        state = action*np.random.randn(2, 2)\n",
    "        reward = np.sum(state)\n",
    "\n",
    "        # done은 numpy의 random.randn 이 0.06 보다 작을 때만 1을 주었습니다. 더 자주 done이 발생하도록 하고 싶다면, 0.06을 더 키우면 됩니다.\n",
    "        done = 1 if abs(np.random.randn())<0.06 else 0\n",
    "        return state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer를 정의합니다.\n",
    "class Buffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.state_buffer = np.zeros((buffer_size, 2 ,2))\n",
    "        self.action_buffer = np.zeros(buffer_size)\n",
    "        self.reward_buffer = np.zeros(buffer_size)\n",
    "        self.next_state_buffer = np.zeros((buffer_size, 2 ,2))\n",
    "        self.done_buffer = np.zeros(buffer_size)\n",
    "        self.act_idx_buffer = np.zeros(buffer_size)\n",
    "        \n",
    "        self.store_idx = 0\n",
    "        self.current_size = 0\n",
    "\n",
    "    def store(self, state, action, next_state, reward, done, actor_idx):\n",
    "        self.state_buffer[self.store_idx] = state\n",
    "        self.action_buffer[self.store_idx] = action\n",
    "        self.reward_buffer[self.store_idx] = reward\n",
    "        self.next_state_buffer[self.store_idx] = next_state\n",
    "        self.done_buffer[self.store_idx] = done\n",
    "        self.act_idx_buffer[self.store_idx] = actor_idx\n",
    "        \n",
    "        self.store_idx = (self.store_idx + 1) % self.buffer_size\n",
    "        self.current_size = min(self.current_size+1, self.buffer_size)\n",
    "    \n",
    "    def batch_load(self, batch_size): \n",
    "        indices = np.random.randint(self.store_idx, size=batch_size)  \n",
    "        return dict( \n",
    "                states=self.state_buffer[indices], \n",
    "                actions=self.action_buffer[indices],\n",
    "                rewards=self.reward_buffer[indices],\n",
    "                next_states=self.next_state_buffer[indices], \n",
    "                dones=self.done_buffer[indices],\n",
    "                actindices=self.act_idx_buffer[indices])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor의 역할은 각각 env에서 경험한 것을 buffer에 넘겨주는 역할을 합니다.\n",
    "@ray.remote\n",
    "class Actor:\n",
    "    def __init__(self, learner, actor_idx):\n",
    "        self.env = Env() \n",
    "        self.learner = learner # ray를 통해 공유하는 learner class입니다.\n",
    "        self.actor_idx = actor_idx # 어떤 actor에서 온 데이터인지 보기 위한 변수입니다.\n",
    "\n",
    "    def explore(self):\n",
    "        state = self.env.reset()\n",
    "        # actor는 멈추지 않아도 되기 때문에, 다음과 같이 무한 loop로 exploration하도록 설정\n",
    "        while 1:\n",
    "            time.sleep(0.1) \n",
    "            action = np.random.randint(3) \n",
    "            next_state, reward, done = self.env.step(action) \n",
    "            self.learner.store.remote(state, action, next_state, reward, done, self.actor_idx) \n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = self.env.reset() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공유 Buffer를 통해 학습을 진행하는 Learner를 정의합니다.\n",
    "@ray.remote\n",
    "class Learner:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.memory = Buffer(buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    # __init__에서 정의된 replay buffer에 저장합니다. 이 메소드는 각 actor마다 실행됩니다. \n",
    "    def store(self, state, action, next_state, reward, done, actor_idx):\n",
    "        self.memory.store(state, action, next_state, reward, done, actor_idx)\n",
    "\n",
    "    # 저장된 buffer에서 데이터를 로딩합니다.\n",
    "    def update_network(self):\n",
    "        batch = self.memory.batch_load(self.batch_size)\n",
    "        \n",
    "        ''' update를 하는 부분 '''\n",
    "        \n",
    "        loss = np.random.randn()\n",
    "        buf_current_size = self.memory.store_idx\n",
    "        return loss, batch['states'].shape, batch['actindices'], buf_current_size # 결과를 확인하기 위해서, loss 이외에 몇 가지를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 5000 # Replay Buffer 사이즈\n",
    "batch_size = 16    # Replay Buffer에서 가지고 올 샘플 개수\n",
    "\n",
    "learner = Learner.remote(buffer_size, batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actors = 16 # actor의 개수\n",
    "\n",
    "# num_actors 개수만큼 선언하고, explore 실행. actor라는 변수가 계속 중복이 되지만 실행은 잘 된다.\n",
    "for idx in range(num_actors):\n",
    "    actor = Actor.remote(learner, idx)\n",
    "    actor.explore.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_updates = 100 # learner가 update_network 메소드를 실행하는 횟수\n",
    "\n",
    "for update_idx in range(n_updates): \n",
    "    time.sleep(1) \n",
    "    loss, batch_stat_shape, act_indices, buf_size = ray.get(learner.update_network.remote())\n",
    "    print(f'Number of updates: {update_idx}')\n",
    "    print(f'Loss: {loss}')\n",
    "    print(f'State shape in Batch: {batch_stat_shape}')\n",
    "    print(f'Actor index: {act_indices}')\n",
    "    print(f'Buffer store index: {buf_size}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Loss: random한 실수값 <br>\n",
    "- State shape: (batch, state[0], state[1])의 자원을 가지는 출력 <br>\n",
    "- Actor index: batch 안의 각 sample이 어느 actor에게 나온 것인지 출력 <br>\n",
    "- Buffer store index: Buffer에 저장되는 현재 store index(각 update 사이에 얼마나 저장되었는지)를 출력  <br><br>\n",
    "\n",
    "#### 대략 아래와 같은 결과가 나오면 의도대로 나온 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Number of updates: 9\n",
    "    Loss: -1.7283143861676746\n",
    "    State shape in Batch: (16, 2, 2)\n",
    "    Actor index: [ 4. 12.  1.  3.  4.  4.  1. 14.  2. 15. 11.  0.  1. 15. 15.  9.]\n",
    "    Buffer store index: 1863\n",
    "\n",
    "    Number of updates: 10\n",
    "    Loss: -1.3466382853532786\n",
    "    State shape in Batch: (16, 2, 2)\n",
    "    Actor index: [ 9.  8. 13. 15. 14.  9.  0.  4.  2.  8. 13.  7.  2.  2.  0. 11.]\n",
    "    Buffer store index: 2023\n",
    "\n",
    "    Number of updates: 11\n",
    "    Loss: -0.8023523911669711\n",
    "    State shape in Batch: (16, 2, 2)\n",
    "    Actor index: [ 3.  9.  9.  7. 12.  3. 12.  6. 12.  5. 10.  7.  0. 11.  3.  6.]\n",
    "    Buffer store index: 2181"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
