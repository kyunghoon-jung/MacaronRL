{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여기서는 Tutorial에서 배운 개념을 이용하여 간단하게 ReplayBuffer를 분산 환경에서 활용해보겠습니다. <br> 아래와 같은 작업을 수행합니다. <br>  \n",
    "    1. 여럿의 agent(혹은 actor)가 공유 Replay Buffer에 경험데이터를 넣는다. \n",
    "    2. Learner는 batch만큼 그 공유 ReplayBuffer에서 load한 후 원하는 작업을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 질문 <br>\n",
    "\n",
    "    1. Class의 method는 공유가 잘 되는데, class 안에 있는 __init__ 에서 선언된 variable은 불러올 수가 없었다. 어떻게 해야하는 걸까?\n",
    "       : 현재로써는 class 안에 변수를 전달하는 method를 따로 만들어서 ray.get으로 접근하는 방법을 쓰고있음.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray \n",
    "import time \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-28 18:13:07,511\tINFO services.py:1173 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.61',\n",
       " 'raylet_ip_address': '192.168.0.61',\n",
       " 'redis_address': '192.168.0.61:44383',\n",
       " 'object_store_address': '/tmp/ray/session_2021-01-28_18-13-06_812268_99563/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-01-28_18-13-06_812268_99563/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8266',\n",
       " 'session_dir': '/tmp/ray/session_2021-01-28_18-13-06_812268_99563',\n",
       " 'metrics_export_port': 56253,\n",
       " 'node_id': 'bee19ef478404e9e4b5159c0980835a49d4e561d'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 env를 정의하겠습니다. environment의 일반적인 메소드만 넣고 어떤 의미가 있는 행동이나 상태를 정의한 것은 아닙니다.\n",
    "class Env:        \n",
    "    def reset(self):\n",
    "        return np.ones((2,2))\n",
    "\n",
    "    def step(self, action):\n",
    "        # state, reward, done 모두 random하게 지정. state의 크기는 2x2 차원을 가지는 2차원 matrix.\n",
    "        state = action*np.random.randn(2, 2)\n",
    "        reward = np.sum(state)\n",
    "\n",
    "        # done은 numpy의 random.randn 이 0.06 보다 작을 때만 1을 주었습니다. 더 자주 done이 발생하도록 하고 싶다면, 0.06을 더 키우면 됩니다.\n",
    "        done = 1 if abs(np.random.randn())<0.06 else 0\n",
    "        return state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer를 정의합니다.\n",
    "@ray.remote\n",
    "class Buffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.state_buffer = np.zeros((buffer_size, 2 ,2))\n",
    "        self.action_buffer = np.zeros(buffer_size)\n",
    "        self.reward_buffer = np.zeros(buffer_size)\n",
    "        self.next_state_buffer = np.zeros((buffer_size, 2 ,2))\n",
    "        self.done_buffer = np.zeros(buffer_size)\n",
    "        self.act_idx_buffer = np.zeros(buffer_size)\n",
    "        \n",
    "        self.store_idx = 0\n",
    "        self.current_size = 0\n",
    "        self.total_store_count = 0\n",
    "\n",
    "    def store(self, state, action, next_state, reward, done, actor_idx):\n",
    "        self.state_buffer[self.store_idx] = state\n",
    "        self.action_buffer[self.store_idx] = action\n",
    "        self.reward_buffer[self.store_idx] = reward\n",
    "        self.next_state_buffer[self.store_idx] = next_state\n",
    "        self.done_buffer[self.store_idx] = done\n",
    "        self.act_idx_buffer[self.store_idx] = actor_idx\n",
    "        \n",
    "        self.store_idx = (self.store_idx + 1) % self.buffer_size\n",
    "        self.current_size = min(self.current_size+1, self.buffer_size)\n",
    "        self.total_store_count += 1\n",
    "    \n",
    "    def batch_load(self, batch_size): \n",
    "        indices = np.random.randint(self.current_size, size=batch_size)  \n",
    "        return dict( \n",
    "                states=self.state_buffer[indices], \n",
    "                actions=self.action_buffer[indices],\n",
    "                rewards=self.reward_buffer[indices],\n",
    "                next_states=self.next_state_buffer[indices], \n",
    "                dones=self.done_buffer[indices],\n",
    "                actindices=self.act_idx_buffer[indices])  \n",
    "    \n",
    "    def return_current_size(self):\n",
    "        return self.total_store_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor의 역할은 각각 env에서 경험한 것을 buffer에 넘겨주는 역할을 합니다.\n",
    "@ray.remote\n",
    "class Actor:\n",
    "    def __init__(self, memory, actor_idx):\n",
    "        self.env = Env() \n",
    "        self.memory = memory # ray를 통해 공유하는 learner class입니다.\n",
    "        self.actor_idx = actor_idx # 어떤 actor에서 온 데이터인지 보기 위한 변수입니다.\n",
    "\n",
    "    def explore(self):\n",
    "        state = self.env.reset()\n",
    "        # actor는 멈추지 않아도 되기 때문에, 다음과 같이 무한 loop로 exploration하도록 설정\n",
    "        while 1:\n",
    "            action = np.random.randint(3) \n",
    "            next_state, reward, done = self.env.step(action) \n",
    "            \n",
    "            # 공유 메모리에 데이터를 저장합니다\n",
    "            self.memory.store.remote(state, action, next_state, reward, done, self.actor_idx) \n",
    "            \n",
    "            state = next_state\n",
    "            time.sleep(0.005)\n",
    "            if done:\n",
    "                state = self.env.reset() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공유 Buffer를 통해 학습을 진행하는 Learner를 정의합니다.\n",
    "class Learner:\n",
    "    def __init__(self, memory, buffer_size, batch_size):\n",
    "        self.memory = memory\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def update_network(self):\n",
    "        # 저장된 buffer에서 데이터를 로딩합니다.\n",
    "        batch = ray.get(self.memory.batch_load.remote(self.batch_size))\n",
    "        print(\"batch is loaded.\")\n",
    "        ''' update를 하는 부분 '''\n",
    "        \n",
    "        loss = np.random.randn()\n",
    "        buffer_store_count = ray.get(self.memory.return_current_size.remote())\n",
    "        return loss, batch['states'].shape, batch['actindices'], buffer_store_count # 결과를 확인하기 위해서, loss 이외에 몇 가지를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 5000 # Replay Buffer 사이즈\n",
    "batch_size = 16    # Replay Buffer에서 가지고 올 샘플 개수\n",
    "\n",
    "memory = Buffer.remote(batch_size)\n",
    "learner = Learner(memory, buffer_size, batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actors = 5 # actor의 개수\n",
    "\n",
    "# num_actors 개수만큼 선언하고, explore 실행. actor라는 변수가 계속 중복이 되지만 실행은 잘 된다.\n",
    "for idx in range(num_actors):\n",
    "    actor = Actor.remote(memory, idx)\n",
    "    actor.explore.remote()\n",
    "time.sleep(1) # 잠시 actor가 어느정도 쌓을 때까지 대기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch is loaded.\n",
      "Number of updates: 0\n",
      "Loss: 1.4752570594307386\n",
      "State shape in Batch: (16, 2, 2)\n",
      "Actor index: [3. 3. 1. 3. 4. 0. 3. 3. 0. 3. 3. 4. 1. 0. 3. 3.]\n",
      "Buffer store index: 773\n",
      "\n",
      "batch is loaded.\n",
      "Number of updates: 1\n",
      "Loss: 0.41636878786804854\n",
      "State shape in Batch: (16, 2, 2)\n",
      "Actor index: [0. 2. 2. 3. 2. 3. 2. 4. 4. 0. 0. 2. 2. 3. 0. 3.]\n",
      "Buffer store index: 1564\n",
      "\n",
      "batch is loaded.\n",
      "Number of updates: 2\n",
      "Loss: 0.21922156767472734\n",
      "State shape in Batch: (16, 2, 2)\n",
      "Actor index: [3. 2. 2. 3. 0. 0. 1. 1. 4. 0. 4. 3. 3. 3. 3. 2.]\n",
      "Buffer store index: 2412\n",
      "\n",
      "batch is loaded.\n",
      "Number of updates: 3\n",
      "Loss: -0.05244928260209676\n",
      "State shape in Batch: (16, 2, 2)\n",
      "Actor index: [4. 4. 3. 0. 4. 4. 4. 4. 1. 0. 1. 3. 4. 0. 3. 0.]\n",
      "Buffer store index: 2913\n",
      "\n",
      "batch is loaded.\n",
      "Number of updates: 4\n",
      "Loss: 1.6871242788143255\n",
      "State shape in Batch: (16, 2, 2)\n",
      "Actor index: [4. 0. 0. 4. 0. 0. 4. 3. 2. 4. 1. 1. 0. 2. 4. 3.]\n",
      "Buffer store index: 3229\n",
      "\n",
      "batch is loaded.\n",
      "Number of updates: 5\n",
      "Loss: -1.9292772107110407\n",
      "State shape in Batch: (16, 2, 2)\n",
      "Actor index: [3. 4. 4. 2. 1. 0. 3. 4. 3. 0. 2. 0. 2. 4. 2. 4.]\n",
      "Buffer store index: 3435\n",
      "\n",
      "batch is loaded.\n",
      "Number of updates: 6\n",
      "Loss: 0.5722745589369793\n",
      "State shape in Batch: (16, 2, 2)\n",
      "Actor index: [2. 4. 3. 3. 2. 1. 0. 1. 3. 3. 2. 1. 3. 2. 2. 2.]\n",
      "Buffer store index: 3574\n",
      "\n",
      "batch is loaded.\n",
      "Number of updates: 7\n",
      "Loss: -0.6602736991116975\n",
      "State shape in Batch: (16, 2, 2)\n",
      "Actor index: [4. 1. 1. 2. 4. 4. 1. 0. 3. 1. 1. 1. 3. 1. 1. 1.]\n",
      "Buffer store index: 3649\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_updates = 100 # learner가 update_network 메소드를 실행하는 횟수\n",
    "\n",
    "for update_idx in range(n_updates): \n",
    "    loss, batch_stat_shape, act_indices, buf_size = learner.update_network()\n",
    "    print(f'Number of updates: {update_idx}')\n",
    "    print(f'Loss: {loss}')\n",
    "    print(f'State shape in Batch: {batch_stat_shape}')\n",
    "    print(f'Actor index: {act_indices}')\n",
    "    print(f'Buffer store index: {buf_size}\\n')\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Loss: random한 실수값 <br>\n",
    "- State shape: (batch, state[0], state[1])의 자원을 가지는 출력 <br>\n",
    "- Actor index: batch 안의 각 sample이 어느 actor에게 나온 것인지 출력 <br>\n",
    "- Buffer store index: Buffer에 저장되는 현재 store index(각 update 사이에 얼마나 저장되었는지)를 출력  <br><br>\n",
    "\n",
    "#### 대략 아래와 같은 결과가 나오면 의도대로 나온 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Number of updates: 9\n",
    "    Loss: -1.7283143861676746\n",
    "    State shape in Batch: (16, 2, 2)\n",
    "    Actor index: [ 4. 12.  1.  3.  4.  4.  1. 14.  2. 15. 11.  0.  1. 15. 15.  9.]\n",
    "    Buffer store index: 1863\n",
    "\n",
    "    Number of updates: 10\n",
    "    Loss: -1.3466382853532786\n",
    "    State shape in Batch: (16, 2, 2)\n",
    "    Actor index: [ 9.  8. 13. 15. 14.  9.  0.  4.  2.  8. 13.  7.  2.  2.  0. 11.]\n",
    "    Buffer store index: 2023\n",
    "\n",
    "    Number of updates: 11\n",
    "    Loss: -0.8023523911669711\n",
    "    State shape in Batch: (16, 2, 2)\n",
    "    Actor index: [ 3.  9.  9.  7. 12.  3. 12.  6. 12.  5. 10.  7.  0. 11.  3.  6.]\n",
    "    Buffer store index: 2181"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
