{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of DQN paper for 2-dimensional Atari games, such as Breakout, Q-bert.\n",
    "#### https://www.nature.com/articles/nature14236\n",
    "#### https://arxiv.org/pdf/1312.5602.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    ''' The same architecture of Q-Network in DQN paper '''\n",
    "    def __init__(self, state_size, action_size, rand_seed=False,\n",
    "                conv_channel_1=32, conv_channel_2=64, conv_channel_3=64,\n",
    "                kernel_1=8, kernel_2=4, kernel_3=3, \n",
    "                stride_1=4, stride_2=2, stride_3=1):\n",
    "\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.Conv1 = nn.Conv2d(state_size[0], conv_channel_1, (kernel_1,kernel_1), stride=stride_1)\n",
    "        self.Conv2 = nn.Conv2d(conv_channel_1, conv_channel_2, (kernel_2,kernel_2), stride=stride_2)\n",
    "        self.Conv3 = nn.Conv2d(conv_channel_2, conv_channel_3, (kernel_3,kernel_3), stride=stride_3)\n",
    "\n",
    "        def calculate_conv2d_size(size, kernel_size, stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "        w, h = state_size[1], state_size[2]\n",
    "        convw = calculate_conv2d_size(calculate_conv2d_size(calculate_conv2d_size(w,kernel_1,stride_1),\n",
    "                                                            kernel_2,stride_2),\n",
    "                                      kernel_3,stride_3)\n",
    "        convh = calculate_conv2d_size(calculate_conv2d_size(calculate_conv2d_size(h,kernel_1,stride_1),\n",
    "                                                            kernel_2,stride_2),\n",
    "                                      kernel_3,stride_3)\n",
    "        linear_input_size = convw * convh * conv_channel_3\n",
    "\n",
    "        self.fc1 = nn.Linear(linear_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, action_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.Conv1(x))\n",
    "        x = self.relu(self.Conv2(x))\n",
    "        x = self.relu(self.Conv3(x))\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    state_size = (4, 84, 84)\n",
    "    action_size = 10\n",
    "    net = QNetwork(state_size, action_size, \n",
    "                   conv_channel_1=32, conv_channel_2=64, conv_channel_3=64)\n",
    "    test = torch.randn(size=(64, 4, 84, 84))\n",
    "    print(net)\n",
    "    print(\"Network output: \", net(test).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\" Experience Replay Buffer in DQN paper \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 buffer_size: ('int: total size of the Replay Buffer'), \n",
    "                 input_dim: ('tuple: a dimension of input data. Ex) (3, 84, 84)'), \n",
    "                 batch_size: ('int: a batch size when updating')):\n",
    "                 \n",
    "        # To check if input image has 3 channels\n",
    "        assert len(input_dim)==3, \"The state dimension should be 3-dim! (CHxWxH). Please check if input_dim is right\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.save_count, self.current_size = 0, 0\n",
    "\n",
    "        # One can choose either np.zeros or np.ones. \n",
    "        # The reason using np.ones here is for checking the total memory occupancy of the buffer. \n",
    "        self.state_buffer = np.ones((buffer_size, input_dim[0], input_dim[1], input_dim[2]), \n",
    "                                    dtype=np.uint8) # data type is np.int8 for saving the memory\n",
    "        self.action_buffer = np.ones(buffer_size, dtype=np.uint8) \n",
    "        self.reward_buffer = np.ones(buffer_size, dtype=np.float32) \n",
    "        self.next_state_buffer = np.ones((buffer_size, input_dim[0], input_dim[1], input_dim[2]),  \n",
    "                                         dtype=np.uint8) \n",
    "        self.done_buffer = np.ones(buffer_size, dtype=np.uint8) \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.current_size\n",
    "\n",
    "    def store(self, \n",
    "              state: np.ndarray, \n",
    "              action: int, \n",
    "              reward: float, \n",
    "              next_state: np.ndarray, \n",
    "              done: int):\n",
    "\n",
    "        self.state_buffer[self.save_count] = state\n",
    "        self.action_buffer[self.save_count] = action\n",
    "        self.reward_buffer[self.save_count] = reward\n",
    "        self.next_state_buffer[self.save_count] = next_state\n",
    "        self.done_buffer[self.save_count] = done\n",
    "\n",
    "        # self.save_count is an index when storing transitions into the replay buffer\n",
    "        self.save_count = (self.save_count + 1) % self.buffer_size\n",
    "        # self.current_size is an indication for how many transitions is stored\n",
    "        self.current_size = min(self.current_size+1, self.buffer_size)\n",
    "\n",
    "    def batch_load(self):\n",
    "        # Selecting samples randomly with a size of self.batch_size \n",
    "        indices = np.random.randint(self.current_size, size=self.batch_size)\n",
    "        return dict(\n",
    "                states=self.state_buffer[indices], \n",
    "                actions=self.action_buffer[indices],\n",
    "                rewards=self.reward_buffer[indices],\n",
    "                next_states=self.next_state_buffer[indices], \n",
    "                dones=self.done_buffer[indices]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>종가</th>\n",
       "      <th>대비</th>\n",
       "      <th>등락률</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>거래량</th>\n",
       "      <th>거래대금</th>\n",
       "      <th>시가총액</th>\n",
       "      <th>상장주식수</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-12</td>\n",
       "      <td>140000</td>\n",
       "      <td>3000</td>\n",
       "      <td>2.19</td>\n",
       "      <td>140500</td>\n",
       "      <td>142500</td>\n",
       "      <td>139500</td>\n",
       "      <td>4548695</td>\n",
       "      <td>6.405300e+11</td>\n",
       "      <td>1.019200e+14</td>\n",
       "      <td>728002365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-11</td>\n",
       "      <td>137000</td>\n",
       "      <td>4000</td>\n",
       "      <td>3.01</td>\n",
       "      <td>134500</td>\n",
       "      <td>139000</td>\n",
       "      <td>134000</td>\n",
       "      <td>6707671</td>\n",
       "      <td>9.187360e+11</td>\n",
       "      <td>9.973630e+13</td>\n",
       "      <td>728002365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-10</td>\n",
       "      <td>133000</td>\n",
       "      <td>-3500</td>\n",
       "      <td>-2.56</td>\n",
       "      <td>140500</td>\n",
       "      <td>140500</td>\n",
       "      <td>132500</td>\n",
       "      <td>5614476</td>\n",
       "      <td>7.634010e+11</td>\n",
       "      <td>9.682430e+13</td>\n",
       "      <td>728002365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-09</td>\n",
       "      <td>136500</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.74</td>\n",
       "      <td>134000</td>\n",
       "      <td>137500</td>\n",
       "      <td>130500</td>\n",
       "      <td>7822152</td>\n",
       "      <td>1.043440e+12</td>\n",
       "      <td>9.937230e+13</td>\n",
       "      <td>728002365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-08</td>\n",
       "      <td>135500</td>\n",
       "      <td>-4500</td>\n",
       "      <td>-3.21</td>\n",
       "      <td>143000</td>\n",
       "      <td>143000</td>\n",
       "      <td>135500</td>\n",
       "      <td>5587787</td>\n",
       "      <td>7.752590e+11</td>\n",
       "      <td>9.864430e+13</td>\n",
       "      <td>728002365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>73100</td>\n",
       "      <td>-7300</td>\n",
       "      <td>-9.08</td>\n",
       "      <td>81600</td>\n",
       "      <td>82500</td>\n",
       "      <td>73000</td>\n",
       "      <td>7949482</td>\n",
       "      <td>6.236650e+11</td>\n",
       "      <td>5.321700e+13</td>\n",
       "      <td>728002365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>80400</td>\n",
       "      <td>-200</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>77500</td>\n",
       "      <td>83100</td>\n",
       "      <td>77000</td>\n",
       "      <td>7107436</td>\n",
       "      <td>5.690690e+11</td>\n",
       "      <td>5.853140e+13</td>\n",
       "      <td>728002365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>80600</td>\n",
       "      <td>-1900</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>83700</td>\n",
       "      <td>84800</td>\n",
       "      <td>80400</td>\n",
       "      <td>5097762</td>\n",
       "      <td>4.209420e+11</td>\n",
       "      <td>5.867700e+13</td>\n",
       "      <td>728002365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>82500</td>\n",
       "      <td>-300</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>76900</td>\n",
       "      <td>84700</td>\n",
       "      <td>76000</td>\n",
       "      <td>8160758</td>\n",
       "      <td>6.518640e+11</td>\n",
       "      <td>6.006020e+13</td>\n",
       "      <td>728002365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>82800</td>\n",
       "      <td>-2700</td>\n",
       "      <td>-3.16</td>\n",
       "      <td>83800</td>\n",
       "      <td>84600</td>\n",
       "      <td>79600</td>\n",
       "      <td>8191818</td>\n",
       "      <td>6.751010e+11</td>\n",
       "      <td>6.027860e+13</td>\n",
       "      <td>728002365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>247 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             일자      종가    대비   등락률      시가      고가      저가      거래량  \\\n",
       "0    2021-03-12  140000  3000  2.19  140500  142500  139500  4548695   \n",
       "1    2021-03-11  137000  4000  3.01  134500  139000  134000  6707671   \n",
       "2    2021-03-10  133000 -3500 -2.56  140500  140500  132500  5614476   \n",
       "3    2021-03-09  136500  1000  0.74  134000  137500  130500  7822152   \n",
       "4    2021-03-08  135500 -4500 -3.21  143000  143000  135500  5587787   \n",
       "..          ...     ...   ...   ...     ...     ...     ...      ...   \n",
       "242  2020-03-18   73100 -7300 -9.08   81600   82500   73000  7949482   \n",
       "243  2020-03-17   80400  -200 -0.25   77500   83100   77000  7107436   \n",
       "244  2020-03-16   80600 -1900 -2.30   83700   84800   80400  5097762   \n",
       "245  2020-03-13   82500  -300 -0.36   76900   84700   76000  8160758   \n",
       "246  2020-03-12   82800 -2700 -3.16   83800   84600   79600  8191818   \n",
       "\n",
       "             거래대금          시가총액      상장주식수  \n",
       "0    6.405300e+11  1.019200e+14  728002365  \n",
       "1    9.187360e+11  9.973630e+13  728002365  \n",
       "2    7.634010e+11  9.682430e+13  728002365  \n",
       "3    1.043440e+12  9.937230e+13  728002365  \n",
       "4    7.752590e+11  9.864430e+13  728002365  \n",
       "..            ...           ...        ...  \n",
       "242  6.236650e+11  5.321700e+13  728002365  \n",
       "243  5.690690e+11  5.853140e+13  728002365  \n",
       "244  4.209420e+11  5.867700e+13  728002365  \n",
       "245  6.518640e+11  6.006020e+13  728002365  \n",
       "246  6.751010e+11  6.027860e+13  728002365  \n",
       "\n",
       "[247 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/home/sonic/IQSL_Projects/cho_gyungmin/data_2229_20210313.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://147.46.44.176:7779/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, \n",
    "                 env: 'Environment',\n",
    "                 input_frame: ('int: The number of channels of input image'),\n",
    "                 input_dim: ('int: The width and height of pre-processed input image'),\n",
    "                 training_frames: ('int: The total number of training frames'),\n",
    "                 skipped_frame: ('int: The number of skipped frames in the environment'),\n",
    "                 eps_decay: ('float: Epsilon Decay_rate'),\n",
    "                 gamma: ('float: Discount Factor'),\n",
    "                 target_update_freq: ('int: Target Update Frequency (by frames)'),\n",
    "                 update_type: ('str: Update type for target network. Hard or Soft')='hard',\n",
    "                 soft_update_tau: ('float: Soft update ratio')=None,\n",
    "                 batch_size: ('int: Update batch size')=32,\n",
    "                 buffer_size: ('int: Replay buffer size')=1000000,\n",
    "                 update_start_buffer_size: ('int: Update starting buffer size')=50000,\n",
    "                 learning_rate: ('float: Learning rate')=0.0004,\n",
    "                 eps_min: ('float: Epsilon Min')=0.1,\n",
    "                 eps_max: ('float: Epsilon Max')=1.0,\n",
    "                 device_num: ('int: GPU device number')=0,\n",
    "                 rand_seed: ('int: Random seed')=None,\n",
    "                 plot_option: ('str: Plotting option')=False,\n",
    "                 model_path: ('str: Model saving path')='./',\n",
    "                 trained_model_path: ('str: Trained model path')=''):\n",
    "\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.device = torch.device(f'cuda:{device_num}' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        self.env = env\n",
    "        self.input_frames = input_frame\n",
    "        self.input_dim = input_dim\n",
    "        self.training_frames = training_frames\n",
    "        self.skipped_frame = skipped_frame\n",
    "        self.epsilon = eps_max\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_min = eps_min\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.update_cnt = 0\n",
    "        self.update_type = update_type\n",
    "        self.tau = soft_update_tau\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.update_start = update_start_buffer_size\n",
    "        self.seed = rand_seed\n",
    "        self.plot_option = plot_option\n",
    "        \n",
    "        self.q_behave = QNetwork((self.input_frames, self.input_dim, self.input_dim), self.action_dim).to(self.device)\n",
    "        self.q_target = QNetwork((self.input_frames, self.input_dim, self.input_dim), self.action_dim).to(self.device)\n",
    "        if trained_model_path: # load a trained model if existing\n",
    "            self.q_behave.load_state_dict(torch.load(trained_model_path))\n",
    "            print(\"Trained model is loaded successfully.\")\n",
    "        \n",
    "        # Initialize target network parameters with behavior network parameters\n",
    "        self.q_target.load_state_dict(self.q_behave.state_dict())\n",
    "        self.q_target.eval()\n",
    "        self.optimizer = optim.Adam(self.q_behave.parameters(), lr=learning_rate) \n",
    "\n",
    "        self.memory = ReplayBuffer(self.buffer_size, (self.input_frames, self.input_dim, self.input_dim), self.batch_size)\n",
    "\n",
    "    def select_action(self, state: 'Must be pre-processed in the same way as updating current Q network. See def _compute_loss'):\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.zeros(self.action_dim), self.env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad(): \n",
    "                # devide an image input with 255 for nomalization\n",
    "                state = torch.FloatTensor(state).to(self.device).unsqueeze(0)/255\n",
    "                Qs = self.q_behave(state)\n",
    "                # take an action of a maximum Q-value\n",
    "                action = Qs.argmax()\n",
    "            \n",
    "            # return action and Q-values (Q-values are not required for implementing algorithms. This is just for checking Q-values for each state. Not must-needed)  \n",
    "            return Qs.detach().cpu().numpy(), action.detach().item()  \n",
    "\n",
    "    def processing_resize_and_gray(self, frame):\n",
    "        ''' Convert images to gray scale and resize ''' \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) \n",
    "        frame = cv2.resize(frame, dsize=(self.input_dim, self.input_dim)).reshape(self.input_dim, self.input_dim).astype(np.uint8) \n",
    "        return frame \n",
    "\n",
    "    def get_init_state(self):\n",
    "        ''' return an initial state with a dimension of (self.input_frames, self.input_dim, self.input_dim) '''\n",
    "        init_state = np.zeros((self.input_frames, self.input_dim, self.input_dim))\n",
    "        init_frame = self.env.reset()\n",
    "        init_state[0] = self.processing_resize_and_gray(init_frame)\n",
    "        \n",
    "        for i in range(1, self.input_frames): \n",
    "            action = self.env.action_space.sample()\n",
    "            for j in range(self.skipped_frame):  \n",
    "                state, _, _, _ = self.env.step(action) \n",
    "            state, _, _, _ = self.env.step(action) \n",
    "            init_state[i] = self.processing_resize_and_gray(state) \n",
    "        return init_state\n",
    "\n",
    "    def get_state(self, state, action, skipped_frame=0):\n",
    "        ''' return reward, next_state, done ''' \n",
    "        next_state = np.zeros((self.input_frames, self.input_dim, self.input_dim))\n",
    "        for i in range(len(state)-1):\n",
    "            next_state[i] = state[i+1]\n",
    "\n",
    "        rewards = 0\n",
    "        dones = 0\n",
    "        \n",
    "        for _ in range(skipped_frame):\n",
    "            state, reward, done, _ = self.env.step(action) \n",
    "            rewards += reward # reward accumulates for the case that rewards occur while skipping\n",
    "            dones += int(done) \n",
    "        state, reward, done, _ = self.env.step(action) \n",
    "        next_state[-1] = self.processing_resize_and_gray(state) \n",
    "        rewards += reward \n",
    "        dones += int(done) \n",
    "        return rewards, next_state, dones\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.memory.store(state, action, reward, next_state, done)\n",
    "\n",
    "    def update_behavior_q_net(self):\n",
    "        # update behavior q network with a batch\n",
    "        batch = self.memory.batch_load()\n",
    "        loss = self._compute_loss(batch)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def target_soft_update(self):\n",
    "        ''' target network is updated with Soft Update. tau is a hyperparameter for the updating ratio betweeen target and behavior network  '''\n",
    "        for target_param, current_param in zip(self.q_target.parameters(), self.q_behave.parameters()):\n",
    "            target_param.data.copy_(self.tau*current_param.data + (1.0-self.tau)*target_param.data)\n",
    "\n",
    "    def target_hard_update(self):\n",
    "        ''' target network is updated with Hard Update '''\n",
    "        self.update_cnt = (self.update_cnt+1) % self.target_update_freq\n",
    "        if self.update_cnt==0:\n",
    "            self.q_target.load_state_dict(self.q_behave.state_dict())\n",
    "\n",
    "    def train(self):\n",
    "        tic = time.time()\n",
    "        losses = []\n",
    "        scores = []\n",
    "        epsilons = []\n",
    "        avg_scores = [[-10000]] # As an initial score, set an arbitrary score of an episode.\n",
    "\n",
    "        score = 0\n",
    "\n",
    "        print(\"Storing initial buffer..\") \n",
    "        state = self.get_init_state()\n",
    "        for frame_idx in range(1, self.update_start+1):\n",
    "            # Store transitions into the buffer until the number of 'self.update_start' transitions is stored \n",
    "            _, action = self.select_action(state)\n",
    "            reward, next_state, done = self.get_state(state, action, skipped_frame=self.skipped_frame)\n",
    "            self.store(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done: state = self.get_init_state()\n",
    "\n",
    "        print(\"Done. Start learning..\")\n",
    "        history_store = []\n",
    "        for frame_idx in range(1, self.training_frames+1):\n",
    "            Qs, action = self.select_action(state)\n",
    "            reward, next_state, done = self.get_state(state, action, skipped_frame=self.skipped_frame)\n",
    "            self.store(state, action, reward, next_state, done)\n",
    "            history_store.append([state, Qs, action, reward, next_state, done]) # history_store is for checking an episode later. Not must-needed.\n",
    "            loss = self.update_behavior_q_net()\n",
    "\n",
    "            if self.update_type=='hard':   self.target_hard_update()\n",
    "            elif self.update_type=='soft': self.target_soft_update()\n",
    "            \n",
    "            score += reward\n",
    "            losses.append(loss)\n",
    "\n",
    "            if done:\n",
    "                # For saving and plotting when an episode is done.\n",
    "                scores.append(score)\n",
    "                if np.mean(scores[-10:]) > max(avg_scores):\n",
    "                    torch.save(self.q_behave.state_dict(), self.model_path+'{}_Score:{}.pt'.format(frame_idx, np.mean(scores[-10:])))\n",
    "                    training_time = round((time.time()-tic)/3600, 1)\n",
    "                    np.save(self.model_path+'{}_history_Score_{}_{}hrs.npy'.format(frame_idx, score, training_time), np.array(history_store))\n",
    "                    print(\"          | Model saved. Recent scores: {}, Training time: {}hrs\".format(scores[-10:], training_time), ' /'.join(os.getcwd().split('/')[-3:]))\n",
    "                avg_scores.append(np.mean(scores[-10:]))\n",
    "\n",
    "                if self.plot_option=='inline': \n",
    "                    scores.append(score)\n",
    "                    epsilons.append(self.epsilon)\n",
    "                    self._plot(frame_idx, scores, losses, epsilons)\n",
    "                else: \n",
    "                    print(score, end='\\r')\n",
    "\n",
    "                score=0\n",
    "                state = self.get_init_state()\n",
    "                history_store = []\n",
    "            else: state = next_state\n",
    "\n",
    "            self._epsilon_step()\n",
    "\n",
    "        print(\"Total training time: {}(hrs)\".format((time.time()-tic)/3600))\n",
    "\n",
    "    def _epsilon_step(self):\n",
    "        ''' Controlling epsilon decay. Here is the same as DQN paper, linearly decaying rate. '''\n",
    "        self.epsilon = max(self.epsilon-self.eps_decay, 0.1)\n",
    "\n",
    "    def _compute_loss(self, batch: \"Dictionary (S, A, R', S', Dones)\"):\n",
    "        ''' Compute loss. If normalization is used, it must be applied to both 'state' and 'next_state'. ex) state/255 '''\n",
    "        states = torch.FloatTensor(batch['states']).to(self.device) / 255\n",
    "        next_states = torch.FloatTensor(batch['next_states']).to(self.device) / 255\n",
    "        actions = torch.LongTensor(batch['actions'].reshape(-1, 1)).to(self.device)\n",
    "        rewards = torch.FloatTensor(batch['rewards'].reshape(-1, 1)).to(self.device)\n",
    "        dones = torch.FloatTensor(batch['dones'].reshape(-1, 1)).to(self.device)\n",
    "\n",
    "        current_q = self.q_behave(states).gather(1, actions)\n",
    "\n",
    "        # target value\n",
    "        next_q = self.q_target(next_states).max(dim=1, keepdim=True)[0].detach()\n",
    "        mask = 1 - dones\n",
    "        target = (rewards + (mask * self.gamma * next_q)).to(self.device)\n",
    "\n",
    "        # Use smooth l1 loss for clipping loss between -1 to 1 as in DQN paper.\n",
    "        loss = F.smooth_l1_loss(current_q, target)\n",
    "        return loss\n",
    "\n",
    "    def _plot(self, frame_idx, scores, losses, epsilons):\n",
    "        clear_output(True) \n",
    "        plt.figure(figsize=(20, 5), facecolor='w') \n",
    "        plt.subplot(131)  \n",
    "        plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
    "        plt.plot(scores) \n",
    "        plt.subplot(132) \n",
    "        plt.title('loss') \n",
    "        plt.plot(losses) \n",
    "        plt.subplot(133) \n",
    "        plt.title('epsilons')\n",
    "        plt.plot(epsilons) \n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_list = {\n",
    "    0: \"CartPole-v0\",\n",
    "    1: \"CartPole-v2\",\n",
    "    2: \"LunarLander-v2\",\n",
    "    3: \"Breakout-v4\",\n",
    "    4: \"BreakoutDeterministic-v4\",\n",
    "    5: \"BreakoutNoFrameskip-v4\",\n",
    "    6: \"BoxingDeterministic-v4\",\n",
    "    7: \"PongDeterministic-v4\",\n",
    "}\n",
    "\n",
    "env_name = env_list[6]\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Same input size as in DQN paper. \n",
    "input_dim = 84\n",
    "input_frame = 4\n",
    "\n",
    "print(\"env_name\", env_name) \n",
    "print(env.unwrapped.get_action_meanings(), env.action_space.n) \n",
    "\n",
    "update_start_buffer_size = 10000\n",
    "training_frames = 10000000\n",
    "eps_max = 1.0\n",
    "eps_min = 0.1\n",
    "eps_decay = 1/1000000\n",
    "gamma = 0.99\n",
    "\n",
    "buffer_size = int(1e6) \n",
    "batch_size = 32           \n",
    "update_type = 'hard'\n",
    "soft_update_tau = 0.002\n",
    "learning_rate = 0.0001\n",
    "target_update_freq = 250\n",
    "skipped_frame = 4\n",
    "\n",
    "device_num = 0\n",
    "rand_seed = None\n",
    "rand_name = ('').join(map(str, np.random.randint(10, size=(3,))))\n",
    "folder_name = os.getcwd().split('/')[-1] \n",
    "\n",
    "model_name = 'Test'\n",
    "model_save_path = f'./model_save/{model_name}/'\n",
    "if not os.path.exists('./model_save/'):\n",
    "    os.mkdir('./model_save/')\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.mkdir(model_save_path)\n",
    "print(\"model_save_path:\", model_save_path)\n",
    "\n",
    "trained_model_path = ''\n",
    "\n",
    "plot_options = {1: 'inline', 2: False} \n",
    "plot_option = plot_options[2]\n",
    "\n",
    "agent = Agent( \n",
    "    env,\n",
    "    input_frame,\n",
    "    input_dim,\n",
    "    training_frames,\n",
    "    skipped_frame,\n",
    "    eps_decay,\n",
    "    gamma,\n",
    "    target_update_freq,\n",
    "    update_type,\n",
    "    soft_update_tau,\n",
    "    batch_size,\n",
    "    buffer_size,\n",
    "    update_start_buffer_size,\n",
    "    learning_rate,\n",
    "    eps_min,\n",
    "    eps_max,\n",
    "    device_num,\n",
    "    rand_seed,\n",
    "    plot_option,\n",
    "    model_save_path,\n",
    "    trained_model_path\n",
    ") \n",
    "\n",
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
